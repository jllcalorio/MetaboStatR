"Paper Title","Paper Link","Publication Year","Publication Type","Publication Title","Author Names","DOI","PDF Link","Open Access","Citations count","Abstract","TL;DR"
"metabolomicsR: a streamlined workflow to analyze metabolomic data in R","https://scispace.com/papers/metabolomicsr-a-streamlined-workflow-to-analyze-metabolomic-3oy1vouo","2022","Journal Article","Bioinformatics advances","Xikun Han
Liming Liang","10.1093/bioadv/vbac067","https://scispace.com/pdf/metabolomicsr-a-streamlined-workflow-to-analyze-metabolomic-3oy1vouo.pdf","Yes","","Abstract Summary metabolomicsR is a streamlined, flexible and user-friendly R package to preprocess, analyze and visualize metabolomic data. metabolomicsR includes comprehensive functionalities for sample and metabolite quality control, outlier detection, missing value imputation, dimensional reduction, batch effect normalization, data integration, regression, metabolite annotation and visualization of data and results. In this application note, we demonstrate the step-by-step use of the main functions from this package. Availability and implementation The metabolomicsR package is available via CRAN and GitHub (https://github.com/XikunHan/metabolomicsR/). A step-by-step online tutorial is available at https://xikunhan.github.io/metabolomicsR/docs/articles/Introduction.html. Supplementary information Supplementary data are available at Bioinformatics Advances online.","A streamlined, flexible and user-friendly R package to preprocess, analyze and visualize metabolomic data that includes comprehensive functionalities for sample and metabolite quality control, outlier detection, missing value imputation, dimensional reduction, batch effect normalization, data integration, regression, metabolite annotation and visualization of data and results."
"Pretreating and normalizing metabolomics data for statistical analysis","https://scispace.com/papers/pretreating-and-normalizing-metabolomics-data-for-5m65h3dj","2023","Journal Article","Genes and Diseases","Jun Sun
Yinglin Xia","10.1016/j.gendis.2023.04.018","","Yes","24","Metabolomics, as a research field and a set of techniques, is to study the entire small molecules in biological samples. Metabolomics is emerging as a powerful tool generally for precision medicine. Particularly, the integration of microbiome and metabolome has revealed the mechanism and functionality of microbiome in human health and disease. However, metabolomics data are very complicated. Preprocessing/pretreating and normalizing procedures on metabolomics data are usually required before statistical analysis. In this review article, we comprehensively review various methods that are used to preprocess and pretreat metabolomics data, including mass spectrometry-based and nuclear magnetic resonance-based data preprocessing, dealing with zero and/or missing values and detecting outliers, data normalization, data centering and scaling, data transformation. We discuss the advantages and limitations of each method. The choice of a suitable preprocessing method is determined by the biological hypothesis, the characteristics of the data set, and the selected statistical data analysis method. We then provide the perspective of their applications in microbiome and metabolome research.","In this article , the authors comprehensively review various methods that are used to preprocess and pretreat metabolomics data, including mass spectrometry-based and nuclear magnetic resonance-based data preprocessing, dealing with zero and/or missing values and detecting outliers, data normalization, data centering and scaling, data transformation."
"A Python-Based Pipeline for Preprocessing LC-MS Data for Untargeted Metabolomics Workflows.","https://scispace.com/papers/a-python-based-pipeline-for-preprocessing-lc-ms-data-for-cyyhbhbs6v","2020","Journal Article","Metabolites","Gabriel Riquelme
Nicolás Zabalegui
Nicolás Zabalegui
Pablo Marchi
Christina M. Jones
María Eugenia Monge","10.3390/METABO10100416","","Yes","37","Preprocessing data in a reproducible and robust way is one of the current challenges in untargeted metabolomics workflows. Data curation in liquid chromatography–mass spectrometry (LC–MS) involves the removal of biologically non-relevant features (retention time, m/z pairs) to retain only high-quality data for subsequent analysis and interpretation. The present work introduces TidyMS, a package for the Python programming language for preprocessing LC–MS data for quality control (QC) procedures in untargeted metabolomics workflows. It is a versatile strategy that can be customized or fit for purpose according to the specific metabolomics application. It allows performing quality control procedures to ensure accuracy and reliability in LC–MS measurements, and it allows preprocessing metabolomics data to obtain cleaned matrices for subsequent statistical analysis. The capabilities of the package are shown with pipelines for an LC–MS system suitability check, system conditioning, signal drift evaluation, and data curation. These applications were implemented to preprocess data corresponding to a new suite of candidate plasma reference materials developed by the National Institute of Standards and Technology (NIST; hypertriglyceridemic, diabetic, and African-American plasma pools) to be used in untargeted metabolomics studies in addition to NIST SRM 1950 Metabolites in Frozen Human Plasma. The package offers a rapid and reproducible workflow that can be used in an automated or semi-automated fashion, and it is an open and free tool available to all users.","The present work introduces TidyMS, a package for the Python programming language for preprocessing LC–MS data for quality control (QC) procedures in untargeted metabolomics workflows, a versatile strategy that can be customized or fit for purpose according to the specific metabolomics application."
"Weighted Scaling Approach for Metabolomics Data Analysis","https://scispace.com/papers/weighted-scaling-approach-for-metabolomics-data-analysis-w4mdpxkh","2022","Journal Article","Japanese Journal of Statistics and Data Science","Biplab Biswas
Nishith Kumar
Md. Aminul Hoque
Md. Ashad Alam","10.48550/arXiv.2208.00603","","No","","Systematic variation is a common issue in metabolomics data analysis. Therefore, different scaling and normalization techniques are used to preprocess the data for metabolomics data analysis. Although several scaling methods are available in the literature, however, choice of scaling, transformation and/or normalization technique influence the further statistical analysis. It is challenging to choose the appropriate scaling technique for downstream analysis to get accurate results or to make a proper decision. Moreover, the existing scaling techniques are sensitive to outliers or extreme values. To fill the gap, our objective is to introduce a robust scaling approach that is not influenced by outliers as well as provides more accurate results for downstream analysis. Here, we introduced a new weighted scaling approach that is robust against outliers however, where no additional outlier detection/treatment step is needed in data preprocessing and also compared it with the conventional scaling and normalization techniques through artificial and real metabolomics datasets. We evaluated the performance of the proposed method in comparison to the other existing conventional scaling techniques using metabolomics data analysis in both the absence and presence of different percentages of outliers. Results show that in most cases, the proposed scaling technique performs better than the traditional scaling methods in both the absence and presence of outliers. The proposed method improves the further downstream metabolomics analysis. The R function of the proposed robust scaling method is available at https://github.com/nishithkumarpaul/robustScaling/blob/main/wscaling.R","This work introduced a new weighted scaling approach that is robust against outliers however, where no additional outlier detection/treatment step is needed in data preprocessing and also compared it with the conventional scaling and normalization techniques through artificial and real metabolomics datasets."
"Weighted Scaling Approach for Metabolomics Data Analysis","https://scispace.com/papers/weighted-scaling-approach-for-metabolomics-data-analysis-xx7za4w1","2022","Posted Content","","","10.48550/arxiv.2208.00603","http://arxiv.org/pdf/2208.00603","Yes","","Systematic variation is a common issue in metabolomics data analysis. Therefore, different scaling and normalization techniques are used to preprocess the data for metabolomics data analysis. Although several scaling methods are available in the literature, however, choice of scaling, transformation and/or normalization technique influence the further statistical analysis. It is challenging to choose the appropriate scaling technique for downstream analysis to get accurate results or to make a proper decision. Moreover, the existing scaling techniques are sensitive to outliers or extreme values. To fill the gap, our objective is to introduce a robust scaling approach that is not influenced by outliers as well as provides more accurate results for downstream analysis. Here, we introduced a new weighted scaling approach that is robust against outliers however, where no additional outlier detection/treatment step is needed in data preprocessing and also compared it with the conventional scaling and normalization techniques through artificial and real metabolomics datasets. We evaluated the performance of the proposed method in comparison to the other existing conventional scaling techniques using metabolomics data analysis in both the absence and presence of different percentages of outliers. Results show that in most cases, the proposed scaling technique performs better than the traditional scaling methods in both the absence and presence of outliers. The proposed method improves the further downstream metabolomics analysis. The R function of the proposed robust scaling method is available at https://github.com/nishithkumarpaul/robustScaling/blob/main/wscaling.R","In this paper , the authors proposed a robust scaling approach that is not influenced by outliers as well as provides more accurate results for downstream analysis, where no additional outlier detection/treatment step is needed in data preprocessing."
"NMF-Based Approach for Missing Values Imputation of Mass Spectrometry Metabolomics Data","https://scispace.com/papers/nmf-based-approach-for-missing-values-imputation-of-mass-3ahxixiqjd","2021","Journal Article","Molecules","Jingjing Xu
Wang Yuanshan
Xiangnan Xu
Kian Kai Cheng
Daniel Raftery
Jiyang Dong","10.3390/MOLECULES26195787","","Yes","","In mass spectrometry (MS)-based metabolomics, missing values (NAs) may be due to different causes, including sample heterogeneity, ion suppression, spectral overlap, inappropriate data processing, and instrumental errors. Although a number of methodologies have been applied to handle NAs, NA imputation remains a challenging problem. Here, we propose a non-negative matrix factorization (NMF)-based method for NA imputation in MS-based metabolomics data, which makes use of both global and local information of the data. The proposed method was compared with three commonly used methods: k-nearest neighbors (kNN), random forest (RF), and outlier-robust (ORI) missing values imputation. These methods were evaluated from the perspectives of accuracy of imputation, retrieval of data structures, and rank of imputation superiority. The experimental results showed that the NMF-based method is well-adapted to various cases of data missingness and the presence of outliers in MS-based metabolic profiles. It outperformed kNN and ORI and showed results comparable with the RF method. Furthermore, the NMF method is more robust and less susceptible to outliers as compared with the RF method. The proposed NMF-based scheme may serve as an alternative NA imputation method which may facilitate biological interpretations of metabolomics data.","In this paper, a non-negative matrix factorization (NMF)-based method for missing values (NAs) in MS-based metabolomics data, which makes use of both global and local information of the data, was proposed."
"MetaboLink: A web application for Streamlined Processing and Analysis of Large-Scale Untargeted Metabolomics Data","https://scispace.com/papers/metabolink-a-web-application-for-streamlined-processing-and-4nnv4wkmia","2024","Journal Article","Bioinformatics","G. Jesus
Jesper F. Havelund
Jonas Lemvig
Veit Schwämmle
Nils J. Færgeman","10.1093/bioinformatics/btae459","","No","","The post-processing and analysis of large-scale untargeted metabolomics data face significant challenges due to the intricate nature of correction, filtration, imputation, and normalization steps. Manual execution across various applications often leads to inefficiencies, human-induced errors, and inconsistencies within the workflow.","MetaboLink, a novel web application designed to process LC-MS metabolomics datasets combining established methodologies and offering flexibility and ease of implementation, is introduced."
"QComics: Recommendations and Guidelines for Robust, Easily Implementable and Reportable Quality Control of Metabolomics Data.","https://scispace.com/papers/qcomics-recommendations-and-guidelines-for-robust-easily-q771w8v1v5","2024","Journal Article","Analytical Chemistry","Alvaro González-Dominguez
Núria Estanyol-Torres
Carl Brunius
Rikard Landberg
Raúl González-Domínguez","10.1021/acs.analchem.3c03660","","No","","The implementation of quality control strategies is crucial to ensure the reproducibility, accuracy, and meaningfulness of metabolomics data. However, this pivotal step is often overlooked within the metabolomics workflow and frequently relies on the use of nonstandardized and poorly reported protocols. To address current limitations in this respect, we have developed QComics, a robust, easily implementable and reportable method for monitoring and controlling data quality. The protocol operates in various sequential steps aimed to (i) correct for background noise and carryover, (ii) detect signal drifts and ""out-of-control"" observations, (iii) deal with missing data, (iv) remove outliers, (v) monitor quality markers to identify samples affected by improper collection, preprocessing, or storage, and (vi) assess overall data quality in terms of precision and accuracy. Notably, this tool considers important issues often neglected along quality control, such as the need of separately handling missing values and truly absent data to avoid losing relevant biological information, as well as the large impact that preanalytical factors may elicit on metabolomics results. Altogether, the guidelines compiled in QComics might contribute to establishing gold standard recommendations and best practices for quality control within the metabolomics community.","QComics provides robust, easily implementable and reportable guidelines for quality control of metabolomics data, ensuring reproducibility, accuracy and meaningfulness."
"Missing data imputation using a truncated infinite factor model with
  application to metabolomics data","https://scispace.com/papers/missing-data-imputation-using-a-truncated-infinite-factor-5zbgn0b8dktm","2024","Journal Article","","Kate Finucane
Lorraine Brennan
Isobel Claire Gormley","10.48550/arxiv.2410.10633","https://scispace.compdf/missing-data-imputation-using-a-truncated-infinite-factor-5zbgn0b8dktm.pdf","No","","In metabolomics, the study of small molecules in biological samples, data are often acquired through mass spectrometry. The resulting data contain highly correlated variables, typically with a larger number of variables than observations. Missing data are prevalent, and imputation is critical as data acquisition can be difficult and expensive, and many analysis methods necessitate complete data. In such data, missing at random (MAR) missingness occurs due to acquisition or processing error, while missing not at random (MNAR) missingness occurs when true values lie below the threshold for detection. Existing imputation methods generally assume one missingness type, or impute values outside the physical constraints of the data, which lack utility. A truncated factor analysis model with an infinite number of factors (tIFA) is proposed to facilitate imputation in metabolomics data, in a statistically and physically principled manner. Truncated distributional assumptions underpin tIFA, ensuring cognisance of the data's physical constraints when imputing. Further, tIFA allows for both MAR and MNAR missingness, and a Bayesian inferential approach provides uncertainty quantification for imputed values and missingness types. The infinite factor model parsimoniously models the high-dimensional, multicollinear data, with nonparametric shrinkage priors obviating the need for model selection tools to infer the number of latent factors. A simulation study is performed to assess the performance of tIFA and an application to a urinary metabolomics dataset results in a full dataset with practically useful imputed values, and associated uncertainty, ready for use in metabolomics analyses. Open-source R code accompanies tIFA, facilitating its widespread use.","A truncated infinite factor model (tIFA) is proposed for missing data imputation in metabolomics, accounting for both MAR and MNAR missingness, and physical constraints, with a Bayesian approach and nonparametric shrinkage priors for uncertainty quantification and model parsimony."
"imputomics: web server and R package for missing values imputation in metabolomics data.","https://scispace.com/papers/imputomics-web-server-and-r-package-for-missing-values-2byj5rkp20","2024","Journal Article","Bioinformatics","Jarosław Chilimoniuk
Krystyna Grzesiak
Jakub Kała
Dominik Nowakowski
Adam Kre Towski
Rafał Kolenda
Michał Ciborowski
Michał Burdukiewicz","10.1093/bioinformatics/btae098","","No","","MOTIVATION Missing values are commonly observed in metabolomics data from mass spectrometry (MS). Imputing them is crucial because it assures data completeness, increases the statistical power of analyses, prevents inaccurate results, and improves the quality of exploratory analysis, statistical modeling, and machine learning. Numerous Missing Value Imputation Algorithms (MVIAs) employ heuristics or statistical models to replace missing information with estimates. In the context of metabolomics data, we identified 52 MVIAs implemented across 70 R functions. Nevertheless, the usage of those 52 established methods poses challenges due to package dependency issues, lack of documentation and their instability. RESULTS Our R package, imputomics, provides a convenient wrapper around 41 (plus random imputation as a baseline model) out of 52 MVIAs in the form of a command-line tool and a web application. In addition, we propose a novel functionality for selecting MVIAs recommended for metabolomics data with the best performance or execution time. AVAILABILITY imputomics is freely available as an R package (github.com/BioGenies/imputomics) and a Shiny web application (biogenies.info/imputomics-ws). The documentation is available at biogenies.info/imputomics. SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.","The R package, imputomics, provides a convenient wrapper around 41 (plus random imputation as a baseline model) out of 52 MVIAs out of 52 MVIAs in the form of a command-line tool and a web application and proposes a novel functionality for selecting MVIAs recommended for metabolomics data with the best performance or execution time."
"Kernel weighted least square approach for imputing missing values of metabolomics data.","https://scispace.com/papers/kernel-weighted-least-square-approach-for-imputing-missing-9hgmkt3bgp","2021","Journal Article","Scientific Reports","Nishith Kumar
Md. Aminul Hoque
Masahiro Sugimoto
Masahiro Sugimoto","10.1038/S41598-021-90654-0","https://scispace.com/pdf/kernel-weighted-least-square-approach-for-imputing-missing-9hgmkt3bgp.pdf","Yes","","Mass spectrometry is a modern and sophisticated high-throughput analytical technique that enables large-scale metabolomic analyses It yields a high-dimensional large-scale matrix (samples × metabolites) of quantified data that often contain missing cells in the data matrix as well as outliers that originate for several reasons, including technical and biological sources Although several missing data imputation techniques are described in the literature, all conventional existing techniques only solve the missing value problems They do not relieve the problems of outliers Therefore, outliers in the dataset decrease the accuracy of the imputation We developed a new kernel weight function-based proposed missing data imputation technique that resolves the problems of missing values and outliers We evaluated the performance of the proposed method and other conventional and recently developed missing imputation techniques using both artificially generated data and experimentally measured data analysis in both the absence and presence of different rates of outliers Performances based on both artificial data and real metabolomics data indicate the superiority of our proposed kernel weight-based missing data imputation technique to the existing alternatives For user convenience, an R package of the proposed kernel weight-based missing value imputation technique was developed, which is available at https://githubcom/NishithPaul/tWLSA","Nishith Paul et al. as mentioned in this paper developed a new kernel weight function based missing data imputation technique that resolves the problems of missing values and outliers by resolving the missing cells in the data matrix as well as outliers that originate from technical and biological sources."
"Addressing Missing Data in GC×GC Metabolomics: Identifying Missingness Type and Evaluating the Impact of Imputation Methods on Experimental Replication","https://scispace.com/papers/addressing-missing-data-in-gcxgc-metabolomics-identifying-2ypw3sqv","2022","Posted Content","","","10.26434/chemrxiv-2021-v9djt-v3","","Yes","","Missing data is a significant issue in metabolomics that is often neglected when conducting data pre-processing, particularly when it comes to imputation. This can have serious implications for downstream statistical analyses and lead to misleading or uninterpretable inferences. In this study, we aim to identify the primary types of missingness that affect untargeted metabolomics data and compare strategies for imputation using two real-world comprehensive two-dimensional gas chromatog-raphy (GC×GC) data sets. We also present these goals in the context of experimental replication whereby imputation is conducted in a within-replicate-based fashion—the first description and evaluation of this strategy—and introduce an R package MetabImpute to carry out these analyses. Our results conclude that, in these two data sets, missingness was most likely of the missing at-random (MAR) and missing not-at-random (MNAR) types as opposed to missing completely at-random (MCAR). Gibbs sampler imputation and Random Forest gave the best results when imputing MAR and MNAR compared against single-value imputation (zero, minimum, mean, median, and half-minimum) and other more sophisticated approach-es (Bayesian principal components analysis and quantile regression imputation for left-censored data). When samples are replicated, within-replicate imputation approaches led to an increase in the reproducibility of peak quantification compared to imputation that ignores replication, suggesting that imputing with respect to replication may preserve potentially important features in downstream analyses for biomarker discovery.","In this paper , the authors identify the primary types of missingness that affect untargeted metabolomics data and compare strategies for imputation using two real-world comprehensive two-dimensional gas chromatography (GC×GC) data sets."
"Addressing Missing Data in GC × GC Metabolomics: Identifying Missingness Type and Evaluating the Impact of Imputation Methods on Experimental Replication","https://scispace.com/papers/addressing-missing-data-in-gc-x-gc-metabolomics-identifying-1ri6obzd","2022","Journal Article","Analytical Chemistry","Vishal Jhanji
Margarita N. Favorskaya","10.1021/acs.analchem.1c04093","","No","","Missing data is a significant issue in metabolomics that is often neglected when conducting data preprocessing, particularly when it comes to imputation. This can have serious implications for downstream statistical analyses and lead to misleading or uninterpretable inferences. In this study, we aim to identify the primary types of missingness that affect untargeted metabolomics data and compare strategies for imputation using two real-world comprehensive two-dimensional gas chromatography (GC × GC) data sets. We also present these goals in the context of experimental replication whereby imputation is conducted in a within-replicate-based fashion─the first description and evaluation of this strategy─and introduce an R package MetabImpute to carry out these analyses. Our results conclude that, in these two GC × GC data sets, missingness was most likely of the missing at-random (MAR) and missing not-at-random (MNAR) types as opposed to missing completely at-random (MCAR). Gibbs sampler imputation and Random Forest gave the best results when imputing MAR and MNAR compared against single-value imputation (zero, minimum, mean, median, and half-minimum) and other more sophisticated approaches (Bayesian principal component analysis and quantile regression imputation for left-censored data). When samples are replicated, within-replicate imputation approaches led to an increase in the reproducibility of peak quantification compared to imputation that ignores replication, suggesting that imputing with respect to replication may preserve potentially important features in downstream analyses for biomarker discovery.","In this paper , the primary types of missingness that affect untargeted metabolomics data and compare strategies for imputation using two real-world comprehensive two-dimensional gas chromatography (GC × GC) data sets."
"Data normalization strategies in metabolomics: Current challenges, approaches, and tools:","https://scispace.com/papers/data-normalization-strategies-in-metabolomics-current-fnez2lddfz","2020","Journal Article","European Journal of Mass Spectrometry","Biswapriya B. Misra","10.1177/1469066720918446","","No","77","Data normalization is a big challenge in quantitative metabolomics approaches, whether targeted or untargeted. Without proper normalization, the mass-spectrometry and spectroscopy data can provide erroneous, sub-optimal data, which can lead to misleading and confusing biological results and thereby result in failed application to human healthcare, clinical, and other research avenues. To address this issue, a number of statistical approaches and software tools have been proposed in the literature and implemented over the years, thereby providing a multitude of approaches to choose from - either sample-based or data-based normalization strategies. In recent years, new dedicated software tools for metabolomics data normalization have surfaced as well. In this account article, I summarize the existing approaches and the new discoveries and research findings in this area of metabolomics data normalization, and I introduce some recent tools that aid in data normalization.","The existing approaches and the new discoveries and research findings in this area of metabolomics data normalization are summarized, and some recent tools that aid in datanormalization are introduced."
"Transformed and filtered metabolite abundances","https://scispace.com/papers/transformed-and-filtered-metabolite-abundances-1tdtf9r8ts","2022","Dataset","","Haley Chatelaine","10.6084/m9.figshare.19233540","","No","","Metabolites with RSD &lt; 30% in pooled QCs were removed, yielding 365 metabolites in positive mode and 414 in negative mode. Metabolite intensities were normalized by sample weight and total ion count, and signal drift corrected with QC-RSC. Missing values were imputed with half-minimum values, and intensities were log2-transformed and auto-scaled before merging positive and negative datasets. Duplicate metabolites were removed using a workflow described in the publication.","Metabolite abundances were transformed and filtered, yielding a dataset of 365 and 414 metabolites in positive and negative modes, respectively. The dataset was processed for normalization, drift correction, imputation, and merging, resulting in a final dataset suitable for downstream analysis."
"Optimization of Imputation Strategies for High-Resolution Gas Chromatography-Mass Spectrometry (HR GC-MS) Metabolomics Data","https://scispace.com/papers/optimization-of-imputation-strategies-for-high-resolution-ybdxmtwu","2022","Posted Content","","","10.20944/preprints202204.0106.v1","","Yes","","Gas chromatography-coupled mass spectrometry (GC-MS) has been used in biomedical research to analyze volatile, non-polar, and polar metabolites in a wide array of sample types. Despite advances in technology, missing values are still common in metabolomics datasets and must be properly handled. We evaluated the performance of ten commonly used missing value imputation methods with metabolites analyzed on an HR GC-MS instrument. By introducing missing values into the complete (i.e., data without any missing values) NIST plasma dataset we demonstrate that Random Forest (RF), Glmnet Ridge Regression (GRR), and Bayesian Principal Component Analysis (BPCA) shared the lowest Root Mean Squared Error (RMSE) in technical replicate data. Further examination of these three methods in data from baboon plasma and liver samples demonstrated they all maintained high accuracy. Overall, our analysis suggests that any of the three imputation methods can be applied effectively to untargeted metabolomics datasets with high accuracy. However, it is important to note that imputation will alter the correlation structure of the dataset, and bias downstream regression coefficients and p-values.","In this article , the authors evaluated the performance of ten commonly used missing value imputation methods with metabolites analyzed on an HR GC-MS instrument and showed that three of them, Random Forest (RF), Glmnet Ridge Regression (GRR), and Bayesian Principal Component Analysis (BPCA), shared the lowest Root Mean Squared Error (RMSE) in technical replicate data."
"Computational Methods for Data Integration and Imputation of Missing Values in <i>Omics</i> Datasets","https://scispace.com/papers/computational-methods-for-data-integration-and-imputation-of-3faon0ix0og7","2024","Journal Article","Proteomics","Yannis Schumann
Antonia Gocke
Julia E. Neumann","10.1002/pmic.202400100","","No","","ABSTRACT Molecular profiling of different omic ‐modalities (e.g., DNA methylomics, transcriptomics, proteomics) in biological systems represents the basis for research and clinical decision‐making. Measurement‐specific biases, so‐called batch effects, often hinder the integration of independently acquired datasets, and missing values further hamper the applicability of typical data processing algorithms. In addition to careful experimental design, well‐defined standards in data acquisition and data exchange, the alleviation of these phenomena particularly requires a dedicated data integration and preprocessing pipeline. This review aims to give a comprehensive overview of computational methods for data integration and missing value imputation for omic data analyses. We provide formal definitions for missing value mechanisms and propose a novel statistical taxonomy for batch effects, especially in the presence of missing data. Based on an automated document search and systematic literature review, we describe 32 distinct data integration methods from five main methodological categories, as well as 37 algorithms for missing value imputation from five separate categories. Additionally, this review highlights multiple quantitative evaluation methods to aid researchers in selecting a suitable set of methods for their work. Finally, this work provides an integrated discussion of the relevance of batch effects and missing values in omics with corresponding method recommendations. We then propose a comprehensive three‐step workflow from the study conception to final data analysis and deduce perspectives for future research. Eventually, we present a comprehensive flow chart as well as exemplary decision trees to aid practitioners in the selection of specific approaches for imputation and data integration in their studies.","This review provides a comprehensive overview of computational methods for data integration and missing value imputation in omics datasets, including 32 data integration methods and 37 imputation algorithms, with a proposed three-step workflow and recommendations for batch effect mitigation."
"Mechanism-aware imputation: a two-step approach in handling missing values in metabolomics","https://scispace.com/papers/mechanism-aware-imputation-a-two-step-approach-in-handling-10e6z4ly","2022","Journal Article","BMC Bioinformatics","J. Dekermanjian
Elin Shaddox
Debmalya Nandy
Debashis Ghosh
Katerina Kechris","10.1186/s12859-022-04659-1","https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/s12859-022-04659-1","Yes","","When analyzing large datasets from high-throughput technologies, researchers often encounter missing quantitative measurements, which are particularly frequent in metabolomics datasets. Metabolomics, the comprehensive profiling of metabolite abundances, are typically measured using mass spectrometry technologies that often introduce missingness via multiple mechanisms: (1) the metabolite signal may be smaller than the instrument limit of detection; (2) the conditions under which the data are collected and processed may lead to missing values; (3) missing values can be introduced randomly. Missingness resulting from mechanism (1) would be classified as Missing Not At Random (MNAR), that from mechanism (2) would be Missing At Random (MAR), and that from mechanism (3) would be classified as Missing Completely At Random (MCAR). Two common approaches for handling missing data are the following: (1) omit missing data from the analysis; (2) impute the missing values. Both approaches may introduce bias and reduce statistical power in downstream analyses such as testing metabolite associations with clinical variables. Further, standard imputation methods in metabolomics often ignore the mechanisms causing missingness and inaccurately estimate missing values within a data set. We propose a mechanism-aware imputation algorithm that leverages a two-step approach in imputing missing values. First, we use a random forest classifier to classify the missing mechanism for each missing value in the data set. Second, we impute each missing value using imputation algorithms that are specific to the predicted missingness mechanism (i.e., MAR/MCAR or MNAR). Using complete data, we conducted simulations, where we imposed different missingness patterns within the data and tested the performance of combinations of imputation algorithms. Our proposed algorithm provided imputations closer to the original data than those using only one imputation algorithm for all the missing values. Consequently, our two-step approach was able to reduce bias for improved downstream analyses.","In this paper , a mechanism-aware imputation algorithm was proposed to estimate missing values within a data set. But this method is limited to a subset of the missing values and cannot handle missing values in a large dataset."
"Mechanism-aware imputation: a two-step approach in handling missing values in metabolomics","https://scispace.com/papers/mechanism-aware-imputation-a-two-step-approach-in-handling-3gz7mj2a","2022","Journal Article","BMC Bioinformatics","J. Dekermanjian
Elin Shaddox
Debmalya Nandy
Debashis Ghosh
Katerina Kechris","10.1186/s12859-022-04659-1","https://scispace.com/pdf/mechanism-aware-imputation-a-two-step-approach-in-handling-3gz7mj2a.pdf","Yes","","When analyzing large datasets from high-throughput technologies, researchers often encounter missing quantitative measurements, which are particularly frequent in metabolomics datasets. Metabolomics, the comprehensive profiling of metabolite abundances, are typically measured using mass spectrometry technologies that often introduce missingness via multiple mechanisms: (1) the metabolite signal may be smaller than the instrument limit of detection; (2) the conditions under which the data are collected and processed may lead to missing values; (3) missing values can be introduced randomly. Missingness resulting from mechanism (1) would be classified as Missing Not At Random (MNAR), that from mechanism (2) would be Missing At Random (MAR), and that from mechanism (3) would be classified as Missing Completely At Random (MCAR). Two common approaches for handling missing data are the following: (1) omit missing data from the analysis; (2) impute the missing values. Both approaches may introduce bias and reduce statistical power in downstream analyses such as testing metabolite associations with clinical variables. Further, standard imputation methods in metabolomics often ignore the mechanisms causing missingness and inaccurately estimate missing values within a data set. We propose a mechanism-aware imputation algorithm that leverages a two-step approach in imputing missing values. First, we use a random forest classifier to classify the missing mechanism for each missing value in the data set. Second, we impute each missing value using imputation algorithms that are specific to the predicted missingness mechanism (i.e., MAR/MCAR or MNAR). Using complete data, we conducted simulations, where we imposed different missingness patterns within the data and tested the performance of combinations of imputation algorithms. Our proposed algorithm provided imputations closer to the original data than those using only one imputation algorithm for all the missing values. Consequently, our two-step approach was able to reduce bias for improved downstream analyses.","In this paper , a mechanism-aware imputation algorithm was proposed to estimate missing values within a data set. But this method is limited to a subset of the missing values and cannot handle missing values in a large dataset."
"From differential abundance to mtGWAS: accurate and scalable methodology for metabolomics data with non-ignorable missing observations and latent factors","https://scispace.com/papers/from-differential-abundance-to-mtgwas-accurate-and-scalable-taz8b3km","2022","","","Shan Zhao
Kedir N. Turi
Tina V. Hartert
Carole Ober
Klaus Bønnelykke
Bo L. Chawes
Hans Bisgaard
Chris McKennan","","","No","","Metabolomics is the high-throughput study of small molecule metabolites. Besides offering novel biological insights, these data contain unique statistical challenges, the most glaring of which is the many non-ignorable missing metabolite observations. To address this issue, nearly all analysis pipelines first impute missing observations, and subsequently perform analyses with methods designed for complete data. While clearly erroneous, these pipelines provide key practical advantages not present in existing statistically rigorous methods, including using both observed and missing data to increase power, fast computation to support phenome- and genome-wide analyses, and streamlined estimates for factor models. To bridge this gap between statistical fidelity and practical utility, we developed MS-NIMBLE, a statistically rigorous and powerful suite of methods that offers all the practical benefits of imputation pipelines to perform phenome-wide differential abundance analyses, metabolite genome-wide association studies (mtGWAS), and factor analysis with non-ignorable missing data. Critically, we tailor MS-NIMBLE to perform differential abundance and mtGWAS in the presence of latent factors, which reduces biases and improves power. In addition to proving its statistical and computational efficiency, we demonstrate its superior performance using three real metabolomic datasets.","This work developed MS-NIMBLE, a statistically rigorous and powerful suite of methods that offers all the practical benefits of imputation pipelines to perform phenome-wide differential abundance analyses, metabolite genome-wide association studies (mtGWAS), and factor analysis with non-ignorable missing data."
"mzQuality: A tool for quality monitoring and reporting of targeted mass spectrometry measurements","https://scispace.com/papers/mzquality-a-tool-for-quality-monitoring-and-reporting-of-7d4vlwqyrmq6","2025","Journal Article","","Marielle van der Peet
Pascal Maas
Agnieszka B. Wegrzyn
Lieke Lamont
Ronan M. T. Fleming
Amy C. Harms
Thomas Hankemeier
Alida Kindt","10.1101/2025.01.22.633547","","No","","Analyzing metabolites using mass spectrometry can offer valuable insight into an individual's health or disease status. However, various sources of experimental variation can affect the data, making robust quality control essential. In this context, we introduce mzQuality, a user-friendly software tool designed to evaluate and correct technical variations in mass spectrometry-based metabolomics data. MzQuality offers key quality control features, such as batch correction, outlier identification, and analysis of signal-to-noise ratios. It supports any peak-integrated processed data independent of vendor software and does not require the user to have any programming skills. We demonstrate the functionality of mzQuality with a data set of 419 samples measured across six batches, in which mzQuality effectively minimized experimental variation, ensuring the data's readiness for statistical analysis and biological interpretation. With customizable settings, mzQuality can be seamlessly integrated into research workflows to produce more accurate and reproducible metabolomics data.","mzQuality is a user-friendly software tool for quality monitoring and reporting of targeted mass spectrometry measurements, offering batch correction, outlier identification, and signal-to-noise ratio analysis, ensuring accurate and reproducible metabolomics data."
"Norm ISWSVR: A Data Integration and Normalization Approach for Large-Scale Metabolomics.","https://scispace.com/papers/norm-iswsvr-a-data-integration-and-normalization-approach-13ofbbyl","2022","Journal Article","Analytical Chemistry","Xian Ding
Fen Yang
Yanhua Chen
Jing Xu
Jiuming He
Rui Zhang
Zeper Abliz","10.1021/acs.analchem.1c05502","","No","","Large-scale and long-period metabolomics study is more susceptible to various sources of systematic errors, resulting in nonreproducibility and poor data quality. A reliable and robust batch correction method removes unwanted systematic variations and improves the statistical power of metabolomics data, which undeniably becomes an important issue for the quality control of metabolomics. This study proposed a novel data normalization and integration method, Norm ISWSVR. It is a two-step approach via combining the best-performance internal standard correction with support vector regression normalization, comprehensively removing the systematic and random errors and matrix effects. This method was investigated in three untargeted lipidomics or metabolomics datasets, and the performance was further evaluated systematically in comparison with that of 11 other normalization methods. As a result, Norm ISWSVR decreased the data's median cross-validated relative standard deviation (cvRSD), increased the correlation between QCs, improved the classification accuracy of biomarkers, and was well-compatible with quantitative data. More importantly, Norm ISWSVR also allows a low frequency of QCs, which could significantly decrease the burden of a large-scale experiment. Correspondingly, Norm ISWSVR favorably improves the data quality of large-scale metabolomics data.","This study proposed a novel data normalization and integration method, Norm ISWSVR, which is a two-step approach via combining the best-performance internal standard correction with support vector regression normalization, comprehensively removing the systematic and random errors and matrix effects."
"Normalization methods for reducing interbatch effect without quality control samples in liquid chromatography-mass spectrometry-based studies","https://scispace.com/papers/normalization-methods-for-reducing-interbatch-effect-without-3z2g48daad","2021","Journal Article","Analytical and Bioanalytical Chemistry","Alisa O. Tokareva
Alisa O. Tokareva
Vitaliy Chagovets
Alexey S. Kononikhin
Natalia L. Starodubtseva
Eugene N. Nikolaev
Vladimir Frankevich","10.1007/S00216-021-03294-8","","No","","Data normalization is an essential part of a large-scale untargeted mass spectrometry metabolomics analysis. Autoscaling, Pareto scaling, range scaling, and level scaling methods for liquid chromatography-mass spectrometry data processing were compared with the most common normalization methods, including quantile normalization, probabilistic quotient normalization, and variance stabilizing normalization. These methods were tested on eight datasets from various clinical studies. The efficiency of the data normalization was assessed by the distance between clusters corresponding to batches and the distance between clusters corresponding to clinical groups in the space of principal components, as well as by the number of features with a pairwise statistically significant difference between the batches and the number of features with a pairwise statistically significant difference between clinical groups. Autoscaling demonstrated the most effective reduction in interbatch variation and can be preferable to probabilistic quotient or quantile normalization in liquid chromatography-mass spectrometry data.","In this article, the authors compared different data normalization methods, including quantile normalization, probabilistic quotient normalization and variance stabilizing normalization on eight datasets from various clinical studies."
"Addressing the batch effect issue for LC/MS metabolomics data in data preprocessing.","https://scispace.com/papers/addressing-the-batch-effect-issue-for-lc-ms-metabolomics-2zrz79p21i","2020","Journal Article","Scientific Reports","Qin Liu
Douglas I. Walker
Karan Uppal
Zihe Liu
Chunyu Ma
ViLinh Tran
Shuzhao Li
Dean P. Jones
Tianwei Yu","10.1038/S41598-020-70850-0","https://scispace.com/pdf/addressing-the-batch-effect-issue-for-lc-ms-metabolomics-2zrz79p21i.pdf","Yes","47","With the growth of metabolomics research, more and more studies are conducted on large numbers of samples. Due to technical limitations of the Liquid Chromatography-Mass Spectrometry (LC/MS) platform, samples often need to be processed in multiple batches. Across different batches, we often observe differences in data characteristics. In this work, we specifically focus on data generated in multiple batches on the same LC/MS machinery. Traditional preprocessing methods treat all samples as a single group. Such practice can result in errors in the alignment of peaks, which cannot be corrected by post hoc application of batch effect correction methods. In this work, we developed a new approach that address the batch effect issue in the preprocessing stage, resulting in better peak detection, alignment and quantification. It can be combined with down-stream batch effect correction methods to further correct for between-batch intensity differences. The method is implemented in the existing workflow of the apLCMS platform. Analyzing data with multiple batches, both generated from standardized quality control (QC) plasma samples and from real biological studies, the new method resulted in feature tables with better consistency, as well as better down-stream analysis results. The method can be a useful addition to the tools available for large studies involving multiple batches. The method is available as part of the apLCMS package. Download link and instructions are at https://mypage.cuhk.edu.cn/academics/yutianwei/apLCMS/ .","A new approach is developed that address the batch effect issue in the preprocessing stage, resulting in better peak detection, alignment and quantification, and can be combined with down-stream batch effect correction methods to further correct for between-batch intensity differences."
"To impute or not to impute in untargeted metabolomics - that is the compositional question","https://scispace.com/papers/to-impute-or-not-to-impute-in-untargeted-metabolomics-that-4ao3i3f0z96v","2024","Journal Article","","Dennis Dimitri Krutkin
Sydney P. Thomas
Simone Zuffa
Prajit Rajkumar
Rob Knight
Pieter C. Dorrestein
Scott T. Kelley","10.1101/2024.10.28.620738","https://scispace.compdf/to-impute-or-not-to-impute-in-untargeted-metabolomics-that-4ao3i3f0z96v.pdf","No","","Untargeted metabolomics often produce large datasets with missing values, arising from biological or technical factors, which can undermine statistical analyses and lead to biased biological interpretations. Imputation methods, such as k- Nearest Neighbors (kNN) and Random Forest (RF) regression are commonly used but their effects vary depending on the type of missing data e.g. Missing Completely At Random (MCAR) and Missing Not At Random (MNAR). Here, we determined the impacts of degree and type of missing data on the accuracy of kNN and RF imputation using two datasets: a targeted metabolomic dataset with spiked-in standards and an untargeted metabolomic dataset. We also assessed the effect of compositional data approaches (CoDA), such as the centered log-ratio (CLR) transform, on data interpretation, since these methods are increasingly being used in metabolomics. Overall, we found that kNN and RF performed more accurately when the proportion of missing data across samples for a metabolic feature was low. However, these imputations could not handle MNAR data and generated wildly inflated values or imputed values where none should exist. Furthermore, we show that the proportion of missing values had a strong impact on the accuracy of imputation which affected the interpretation of the results. Our results suggest extreme caution should be used with imputation even with modestly levels of missing data or when the type of missingness is unknown.","This study investigates the impact of missing data on untargeted metabolomics, finding that imputation methods (kNN, RF) perform well with low missing data rates but fail with Missing Not At Random data, highlighting the need for caution in data interpretation."
"IP4M: an integrated platform for mass spectrometry-based metabolomics data mining","https://scispace.com/papers/ip4m-an-integrated-platform-for-mass-spectrometry-based-5c0xpbjj4m","2020","Journal Article","BMC Bioinformatics","Dandan Liang
Quan Liu
Kejun Zhou
Wei Jia
Guoxiang Xie
Tianlu Chen","10.1186/S12859-020-03786-X","https://scispace.com/pdf/ip4m-an-integrated-platform-for-mass-spectrometry-based-5c0xpbjj4m.pdf","Yes","44","Metabolomics data analyses rely on the use of bioinformatics tools. Many integrated multi-functional tools have been developed for untargeted metabolomics data processing and have been widely used. More alternative platforms are expected for both basic and advanced users. Integrated mass spectrometry-based untargeted metabolomics data mining (IP4M) software was designed and developed. The IP4M, has 62 functions categorized into 8 modules, covering all the steps of metabolomics data mining, including raw data preprocessing (alignment, peak de-convolution, peak picking, and isotope filtering), peak annotation, peak table preprocessing, basic statistical description, classification and biomarker detection, correlation analysis, cluster and sub-cluster analysis, regression analysis, ROC analysis, pathway and enrichment analysis, and sample size and power analysis. Additionally, a KEGG-derived metabolic reaction database was embedded and a series of ratio variables (product/substrate) can be generated with enlarged information on enzyme activity. A new method, GRaMM, for correlation analysis between metabolome and microbiome data was also provided. IP4M provides both a number of parameters for customized and refined analysis (for expert users), as well as 4 simplified workflows with few key parameters (for beginners who are unfamiliar with computational metabolomics). The performance of IP4M was evaluated and compared with existing computational platforms using 2 data sets derived from standards mixture and 2 data sets derived from serum samples, from GC–MS and LC–MS respectively. IP4M is powerful, modularized, customizable and easy-to-use. It is a good choice for metabolomics data processing and analysis. Free versions for Windows, MAC OS, and Linux systems are provided.","The performance of IP4M was evaluated and compared with existing computational platforms using 2 data sets derived from standards mixture and 2 data set derived from serum samples, from GC–MS and LC–MS respectively."
"MStractor: R Workflow Package for Enhancing Metabolomics Data Pre-Processing and Visualization","https://scispace.com/papers/mstractor-r-workflow-package-for-enhancing-metabolomics-data-19ep5s0gb3","2021","Journal Article","Metabolites","L Nicolotti
Jeremy Hack
Markus Herderich
Natoiya D. R. Lloyd","10.3390/METABO11080492","https://www.mdpi.com/2218-1989/11/8/492/pdf","Yes","","Untargeted metabolomics experiments for characterizing complex biological samples, conducted with chromatography/mass spectrometry technology, generate large datasets containing very complex and highly variable information. Many data-processing options are available, however, both commercial and open-source solutions for data processing have limitations, such as vendor platform exclusivity and/or requiring familiarity with diverse programming languages. Data processing of untargeted metabolite data is a particular problem for laboratories that specialize in non-routine mass spectrometry analysis of diverse sample types across humans, animals, plants, fungi, and microorganisms. Here, we present MStractor, an R workflow package developed to streamline and enhance pre-processing of metabolomics mass spectrometry data and visualization. MStractor combines functions for molecular feature extraction with user-friendly dedicated GUIs for chromatographic and mass spectromerty (MS) parameter input, graphical quality-control outputs, and descriptive statistics. MStractor performance was evaluated through a detailed comparison with XCMS Online. The MStractor package is freely available on GitHub at the MetabolomicsSA repository.","MStractor as mentioned in this paper is an R workflow package developed to streamline and enhance pre-processing of metabolomics mass spectrometry data and visualization, which combines functions for molecular feature extraction with user-friendly dedicated GUIs for chromatographic and MS parameter input, graphical quality-control outputs, and descriptive statistics."
"NOREVA: enhanced normalization and evaluation of time-course and multi-class metabolomic data.","https://scispace.com/papers/noreva-enhanced-normalization-and-evaluation-of-time-course-59wo6rbp12","2020","Journal Article","Nucleic Acids Research","Qingxia Yang
Qingxia Yang
Yunxia Wang
Ying Zhang
Fengcheng Li
Weiqi Xia
Ying Zhou
Yunqing Qiu
Honglin Li
Feng Zhu
Feng Zhu","10.1093/NAR/GKAA258","","Yes","175","Biological processes (like microbial growth & physiological response) are usually dynamic and require the monitoring of metabolic variation at different time-points. Moreover, there is clear shift from case-control (N=2) study to multi-class (N>2) problem in current metabolomics, which is crucial for revealing the mechanisms underlying certain physiological process, disease metastasis, etc. These time-course and multi-class metabolomics have attracted great attention, and data normalization is essential for removing unwanted biological/experimental variations in these studies. However, no tool (including NOREVA 1.0 focusing only on case-control studies) is available for effectively assessing the performance of normalization method on time-course/multi-class metabolomic data. Thus, NOREVA was updated to version 2.0 by (i) realizing normalization and evaluation of both time-course and multi-class metabolomic data, (ii) integrating 144 normalization methods of a recently proposed combination strategy and (iii) identifying the well-performing methods by comprehensively assessing the largest set of normalizations (168 in total, significantly larger than those 24 in NOREVA 1.0). The significance of this update was extensively validated by case studies on benchmark datasets. All in all, NOREVA 2.0 is distinguished for its capability in identifying well-performing normalization method(s) for time-course and multi-class metabolomics, which makes it an indispensable complement to other available tools. NOREVA can be accessed at https://idrblab.org/noreva/.","NOREVA 2.0 is distinguished for its capability in identifying well-performing normalization method(s) for time-course and multi-class metabolomics, which makes it an indispensable complement to other available tools."
"Imputation of Missing Values for Multi-Biospecimen Metabolomics Studies: Bias and Effects on Statistical Validity","https://scispace.com/papers/imputation-of-missing-values-for-multi-biospecimen-2l9zbbc3","2022","Journal Article","Metabolites","Machelle D. Wilson
Matthew D. Ponzini
Sandra L. Taylor
Kyoungmi Kim","10.3390/metabo12070671","https://scispace.com/pdf/imputation-of-missing-values-for-multi-biospecimen-2l9zbbc3.pdf","Yes","","The analysis of high-throughput metabolomics mass spectrometry data across multiple biological sample types (biospecimens) poses challenges due to missing data. During differential abundance analysis, dropping samples with missing values can lead to severe loss of data as well as biased results in group comparisons and effect size estimates. However, the imputation of missing data (the process of replacing missing data with estimated values such as a mean) may compromise the inherent intra-subject correlation of a metabolite across multiple biospecimens from the same subject, which in turn may compromise the efficacy of the statistical analysis of differential metabolites in biomarker discovery. We investigated imputation strategies when considering multiple biospecimens from the same subject. We compared a novel, but simple, approach that consists of combining the two biospecimen data matrices (rows and columns of subjects and metabolites) and imputes the two biospecimen data matrices together to an approach that imputes each biospecimen data matrix separately. We then compared the bias in the estimation of the intra-subject multi-specimen correlation and its effects on the validity of statistical significance tests between two approaches. The combined approach to multi-biospecimen studies has not been evaluated previously even though it is intuitive and easy to implement. We examine these two approaches for five imputation methods: random forest, k nearest neighbor, expectation-maximization with bootstrap, quantile regression, and half the minimum observed value. Combining the biospecimen data matrices for imputation did not greatly increase efficacy in conserving the correlation structure or improving accuracy in the statistical conclusions for most of the methods examined. Random forest tended to outperform the other methods in all performance metrics, except specificity.","Combining the biospecimen data matrices for imputation did not greatly increase efficacy in conserving the correlation structure or improving accuracy in the statistical conclusions for most of the methods examined."
"MetaboAnalystR 3.0: Toward an Optimized Workflow for Global Metabolomics.","https://scispace.com/papers/metaboanalystr-3-0-toward-an-optimized-workflow-for-global-2d4fcwcuun","2020","Journal Article","Metabolites","Zhiqiang Pang
Jasmine Chong
Shuzhao Li
Jianguo Xia","10.3390/METABO10050186","https://scispace.com/pdf/metaboanalystr-3-0-toward-an-optimized-workflow-for-global-2d4fcwcuun.pdf","Yes","460","Liquid chromatography coupled to high-resolution mass spectrometry platforms are increasingly employed to comprehensively measure metabolome changes in systems biology and complex diseases. Over the past decade, several powerful computational pipelines have been developed for spectral processing, annotation, and analysis. However, significant obstacles remain with regard to parameter settings, computational efficiencies, batch effects, and functional interpretations. Here, we introduce MetaboAnalystR 3.0, a significantly improved pipeline with three key new features: (1) efficient parameter optimization for peak picking; (2) automated batch effect correction; and 3) more accurate pathway activity prediction. Our benchmark studies showed that this workflow was 20~100X faster compared to other well-established workflows and produced more biologically meaningful results. In summary, MetaboAnalystR 3.0 offers an efficient pipeline to support high-throughput global metabolomics in the open-source R environment.","This work introduces MetaboAnalystR 3.0, a significantly improved pipeline with three key new features: efficient parameter optimization for peak picking; automated batch effect correction; and more accurate pathway activity prediction that offers an efficient pipeline to support high-throughput global metabolomics in the open-source R environment."
"Regularized adversarial learning for normalization of multi-batch untargeted metabolomics data","https://scispace.com/papers/regularized-adversarial-learning-for-normalization-of-multi-12momngr","2023","Journal Article","Bioinformatics","A. Dmitrenko
Michelle Reid
Nicola Zamboni","10.1093/bioinformatics/btad096","https://scispace.com/pdf/regularized-adversarial-learning-for-normalization-of-multi-12momngr.pdf","Yes","","Abstract Motivation Untargeted metabolomics by mass spectrometry is the method of choice for unbiased analysis of molecules in complex samples of biological, clinical or environmental relevance. The exceptional versatility and sensitivity of modern high-resolution instruments allows profiling of thousands of known and unknown molecules in parallel. Inter-batch differences constitute a common and unresolved problem in untargeted metabolomics, and hinder the analysis of multi-batch studies or the intercomparison of experiments. Results We present a new method, Regularized Adversarial Learning Preserving Similarity (RALPS), for the normalization of multi-batch untargeted metabolomics data. RALPS builds on deep adversarial learning with a three-term loss function that mitigates batch effects while preserving biological identity, spectral properties and coefficients of variation. Using two large metabolomics datasets, we showcase the superior performance of RALPS as compared with six state-of-the-art methods for batch correction. Further, we demonstrate that RALPS scales well, is robust, deals with missing values and can handle different experimental designs. Availability and implementation https://github.com/zamboni-lab/RALPS. Supplementary information Supplementary data are available at Bioinformatics online.","Zamboni et al. as discussed by the authors proposed Regularized Adversarial Learning Preserving Similarity (RALPS) for the normalization of multi-batch untargeted metabolomics data, which builds on deep adversarial learning with a threeterm loss function that mitigates batch effects while preserving biological identity, spectral properties and coefficients of variation."
"omicsMIC: a comprehensive benchmarking platform for robust comparison of imputation methods in mass spectrometry-based omics data","https://scispace.com/papers/omicsmic-a-comprehensive-benchmarking-platform-for-robust-2dlumur52d","2024","Journal Article","NAR genomics and bioinformatics","Weiqiang Lin
Jiadong Ji
Kuan-Jui Su
Chuan Qiu
Qing Tian
Lan‐Juan Zhao
Zhe Luo
Chong Wu
Hui Shen
Hong‐Wen Deng","10.1093/nargab/lqae071","","No","","Abstract Mass spectrometry is a powerful and widely used tool for generating proteomics, lipidomics and metabolomics profiles, which is pivotal for elucidating biological processes and identifying biomarkers. However, missing values in mass spectrometry-based omics data may pose a critical challenge for the comprehensive identification of biomarkers and elucidation of the biological processes underlying human complex disorders. To alleviate this issue, various imputation methods for mass spectrometry-based omics data have been developed. However, a comprehensive comparison of these imputation methods is still lacking, and researchers are frequently confronted with a multitude of options without a clear rationale for method selection. To address this pressing need, we developed omicsMIC (mass spectrometry-based omics with Missing values Imputation methods Comparison platform), an interactive platform that provides researchers with a versatile framework to evaluate the performance of 28 diverse imputation methods. omicsMIC offers a nuanced perspective, acknowledging the inherent heterogeneity in biological data and the unique attributes of each dataset. Our platform empowers researchers to make data-driven decisions in imputation method selection based on real-time visualizations of the outcomes associated with different imputation strategies. The comprehensive benchmarking and versatility of omicsMIC make it a valuable tool for the scientific community engaged in mass spectrometry-based omics research. omicsMIC is freely available at https://github.com/WQLin8/omicsMIC.","omicsMIC is a platform for benchmarking imputation methods in mass spectrometry-based omics data, providing a comprehensive comparison of 28 methods and empowering researchers to make data-driven decisions."
"<u>Imp</u>utation for <u>Li</u>pidomics and <u>Met</u>abolomics (ImpLiMet): a Web-based application for optimization and method selection for missing data imputation","https://scispace.com/papers/u-imp-u-utation-for-u-li-u-pidomics-and-u-met-u-abolomics-75lqi0dnb56h","2024","Journal Article","Bioinformatics advances","Huiting Ou
Anuradha Surendra
Graeme S. V. McDowell
Emily Hashimoto-Roth
Jianguo Xia
Steffany A. L. Bennett
Miroslava Čuperlović‐Culf","10.1093/bioadv/vbae209","","No","","Abstract Motivation Missing values are prevalent in high-throughput measurements due to various experimental or analytical reasons. Imputation, the process of replacing missing values in a dataset with estimated values, plays an important role in multivariate and machine learning analyses. The three missingness patterns, including missing completely at random, missing at random, and missing not at random, describe unique dependencies between the missing and observed data. The optimal imputation method for each dataset depends on the type of data, the cause of the missingness, and the nature of relationships between the missing and observed data. The challenge is to identify the optimal imputation solution for a given dataset. Results ImpLiMet: is a user-friendly web-platform that enables users to impute missing data using eight different methods. For a given dataset, ImpLiMet suggests the optimal imputation solution through a grid search-based investigation of the error rate for imputation across three missingness data simulations. The effect of imputation can be visually assessed by histogram, kurtosis, and skewness, as well as principal component analysis comparing the impact of the chosen imputation method on the distribution and overall behavior of the data. Availability and implementation ImpLiMet is freely available at https://complimet.ca/shiny/implimet/ and https://github.com/complimet/ImpLiMet.","ImpLiMet is a web-based application that optimizes missing data imputation in high-throughput measurements by suggesting the best imputation method through a grid search-based investigation of error rates across three missingness simulations."
"omicsMIC: a Comprehensive Benchmarking Platform for Robust Comparison of Imputation Methods in Mass Spectrometry-based Omics Data","https://scispace.com/papers/omicsmic-a-comprehensive-benchmarking-platform-for-robust-1w2iskx6s6","2023","Journal Article","bioRxiv","Weiqiang Lin
Jiadong Ji
Kuan Jui Su
Chuan Qiu
Qing Tian
Lan-Juan Zhao
Zhe Luo
Hui Shen
Chong Wu
Hongwen Deng","10.1101/2023.09.12.557189","","No","","Mass spectrometry is a powerful and widely used tool for generating proteomics, lipidomics, and metabolomics profiles, which is pivotal for elucidating biological processes and identifying biomarkers. However, missing values in spectrometry-based omics data may pose a critical challenge for the comprehensive identification of biomarkers and elucidation of the biological processes underlying human complex disorders. To alleviate this issue, various imputation methods for mass spectrometry-based omics data have been developed. However, a comprehensive and systematic comparison of these imputation methods is still lacking, and researchers are frequently confronted with a multitude of options without a clear rationale for method selection. To address this pressing need, we developed omicsMIC (mass spectrometrybased omics with Missing values Imputation methods Comparison platform), an interactive platform that provides researchers with a versatile framework to simulate and evaluate the performance of 28 diverse imputation methods. omicsMIC offers a nuanced perspective, acknowledging the inherent heterogeneity in biological data and the unique attributes of each dataset. Our platform empowers researchers to make data-driven decisions in imputation method selection based on real-time visualizations of the outcomes associated with different imputation strategies. The comprehensive benchmarking and versatility of omicsMIC make it a valuable tool for the scientific community engaged in mass spectrometry-based omics research. OmicsMIC is freely available at https://github.com/WQLin8/omicsMIC.","OmicsMIC (mass spectrometrybased omics with Missing values Imputation methods Comparison platform), an interactive platform that provides researchers with a versatile framework to simulate and evaluate the performance of 28 diverse imputation methods, is developed."
"Closing the Knowledge Gap of Post-Acquisition Sample Normalization in Untargeted Metabolomics","https://scispace.com/papers/closing-the-knowledge-gap-of-post-acquisition-sample-749ngu95p7hs","2024","Journal Article","ACS Measurement Au","Brian J. Low
Yukai Wang
Tingting Zhao
Huaxu Yu
Tao Huan","10.1021/acsmeasuresciau.4c00047","","No","","Sample normalization is a crucial step in metabolomics for fair quantitative comparisons. It aims to minimize sample-to-sample variations due to differences in the total metabolite amount. When samples lack a specific metabolic quantity to accurately represent their total metabolite amounts, post-acquisition sample normalization becomes essential. Despite many proposed normalization algorithms, understanding remains limited of their differences, hindering the selection of the most suitable one for a given metabolomics study. This study bridges this knowledge gap by employing data simulation, experimental simulation, and real experiments to elucidate the differences in the mechanism and performance among common post-acquisition sample normalization methods. Using public datasets, we first demonstrated the dramatic discrepancies between the outcomes of different sample normalization methods. Then, we benchmarked six normalization methods: sum, median, probabilistic quotient normalization (PQN), maximal density fold change (MDFC), quantile, and class-specific quantile. Our results show that most normalization methods are biased when there is unbalanced data, a phenomenon where the percentages of up- and downregulated metabolites are unequal. Notably, unbalanced data can be sourced from the underlying biological differences, experimental perturbations, and metabolic interference. Beyond normalization algorithms and data structure, our study also emphasizes the importance of considering additional factors contributed by data quality, such as background noise, signal saturation, and missingness. Based on these findings, we propose an evidence-based normalization strategy to maximize sample normalization outcomes, providing a robust bioinformatic solution for advancing metabolomics research with a fair quantitative comparison.","This study bridges the knowledge gap on post-acquisition sample normalization in untargeted metabolomics by comparing six methods, highlighting biases in unbalanced data and proposing an evidence-based strategy to maximize normalization outcomes for fair quantitative comparisons."
"Statistical Depth based Normalization and Outlier Detection of Gene Expression Data","https://scispace.com/papers/statistical-depth-based-normalization-and-outlier-detection-nrkpfule","2022","","","Alicia Nieto-Reyes
Javier Cabrera","","","No","","Normalization and outlier detection belong to the preprocessing of gene expression data. We propose a natural normalization procedure based on statistical data depth which normalizes to the distribution of gene expressions of the most representative gene expression of the group. This differ from the standard method of quantile normalization, based on the coordinate-wise median array that lacks of the well-known properties of the one-dimensional median. The statistical data depth maintains those good properties. Gene expression data are known for containing outliers. Although detecting outlier genes in a given gene expression dataset has been broadly studied, these methodologies do not apply for detecting outlier samples, given the difficulties posed by the high dimensionality but low sample size structure of the data. The standard procedures used for detecting outlier samples are visual and based on dimension reduction techniques; instances are multidimensional scaling and spectral map plots. For detecting outlier genes in a given gene expression dataset, we propose an analytical procedure and based on the Tukey’s concept of outlier and the notion of statistical depth, as previous methodologies lead to unassertive and wrongful outliers. We reveal the outliers of four datasets; as a necessary step for further research.","In this article , the authors proposed a natural normalization procedure based on statistical data depth which normalizes to the distribution of gene expressions of the most representative gene expression of the group, and revealed the outliers of four datasets; as a necessary step for further research."
"Statistical Depth based Normalization and Outlier Detection of Gene
  Expression Data","https://scispace.com/papers/statistical-depth-based-normalization-and-outlier-detection-295xmnkw","2022","Posted Content","","","10.48550/arxiv.2206.13928","https://scispace.com/pdf/statistical-depth-based-normalization-and-outlier-detection-295xmnkw.pdf","Yes","","Normalization and outlier detection belong to the preprocessing of gene expression data. We propose a natural normalization procedure based on statistical data depth which normalizes to the distribution of gene expressions of the most representative gene expression of the group. This differ from the standard method of quantile normalization, based on the coordinate-wise median array that lacks of the well-known properties of the one-dimensional median. The statistical data depth maintains those good properties. Gene expression data are known for containing outliers. Although detecting outlier genes in a given gene expression dataset has been broadly studied, these methodologies do not apply for detecting outlier samples, given the difficulties posed by the high dimensionality but low sample size structure of the data. The standard procedures used for detecting outlier samples are visual and based on dimension reduction techniques; instances are multidimensional scaling and spectral map plots. For detecting outlier genes in a given gene expression dataset, we propose an analytical procedure and based on the Tukey's concept of outlier and the notion of statistical depth, as previous methodologies lead to unassertive and wrongful outliers. We reveal the outliers of four datasets; as a necessary step for further research.","In this article , the authors proposed a natural normalization procedure based on statistical data depth which normalizes to the distribution of gene expressions of the most representative gene expression of the group, and revealed the outliers of four datasets; as a necessary step for further research."
"Data-dependent normalization strategies for untargeted metabolomics—a case study","https://scispace.com/papers/data-dependent-normalization-strategies-for-untargeted-2f2uc8k2gw","2020","Journal Article","Analytical and Bioanalytical Chemistry","Paula Cuevas-Delgado
Danuta Dudzik
Danuta Dudzik
Verónica Miguel
Santiago Lamas
Coral Barbas","10.1007/S00216-020-02594-9","","No","30","Despite the recent advances in the standardization of untargeted metabolomics workflows, there is still a lack of attention to specific data treatment strategies that require deep knowledge of the biological problem and need to be applied after a well-thought out process to understand the effect of the practice. One of those strategies is data normalization. Data-driven assumptions are critical especially addressing unwanted variation present in the biological model as it can be the case in heterogeneous tissues, cells with different sizes or biofluids with different concentrations. Chronic kidney disease (CKD) is a widespread disorder affecting kidney structure and function. Animal models are being developed to be able to get valuable insights into the etiopathogenesis of the condition and effect of the treatments. Moreover, diagnosis and disease staging still require defining appropriate biomarkers. Untargeted metabolomics has the potential to deal with those challenges. Renal fibrosis is one of the consequences of kidney injury which greatly affects the concentration of metabolites in the same quantity of sample. To overcome this challenge, several data normalization strategies have been applied, following a multilevel normalization method with the overall aim of focussing on the relevant biological information and reducing the influence of disturbing factors. A comprehensive evaluation of the performance of the normalization strategies, both on methods assessing the intragroup variation and on the impact on differential analysis, is provided. Finally, we present evidence of the importance of biological-model-driven guided normalization methods and discuss multiple criteria that need to be taken into consideration to obtain robust and reliable data. Special concern is transmitted on the misleading conclusions that might be the consequence of inappropriate data pre-treatment solutions applied for untargeted methods.","Evidence of the importance of biological-model-driven guided normalization methods is presented and multiple criteria that need to be taken into consideration to obtain robust and reliable data are discussed."
"Instrumental Drift in Untargeted Metabolomics: Optimizing Data Quality with Intrastudy QC Samples","https://scispace.com/papers/instrumental-drift-in-untargeted-metabolomics-optimizing-626njri1","2023","Journal Article","Metabolites","Andre Märtens
Johannes Holle
Brit Mollenhauer
Andre Wegner
Jennifer A. Kirwan
Karsten Hiller","10.3390/metabo13050665","https://www.mdpi.com/2218-1989/13/5/665/pdf?version=1684232827","Yes","","Untargeted metabolomics is an important tool in studying health and disease and is employed in fields such as biomarker discovery and drug development, as well as precision medicine. Although significant technical advances were made in the field of mass-spectrometry driven metabolomics, instrumental drifts, such as fluctuations in retention time and signal intensity, remain a challenge, particularly in large untargeted metabolomics studies. Therefore, it is crucial to consider these variations during data processing to ensure high-quality data. Here, we will provide recommendations for an optimal data processing workflow using intrastudy quality control (QC) samples that identifies errors resulting from instrumental drifts, such as shifts in retention time and metabolite intensities. Furthermore, we provide an in-depth comparison of the performance of three popular batch-effect correction methods of different complexity. By using different evaluation metrics based on QC samples and a machine learning approach based on biological samples, the performance of the batch-effect correction methods were evaluated. Here, the method TIGER demonstrated the overall best performance by reducing the relative standard deviation of the QCs and dispersion-ratio the most, as well as demonstrating the highest area under the receiver operating characteristic with three different probabilistic classifiers (Logistic regression, Random Forest, and Support Vector Machine). In summary, our recommendations will help to generate high-quality data that are suitable for further downstream processing, leading to more accurate and meaningful insights into the underlying biological processes.","In this paper , the authors provide recommendations for an optimal data processing workflow using intrastudy quality control (QC) samples that identifies errors resulting from instrumental drifts, such as shifts in retention time and metabolite intensities."
"A hierarchical approach to removal of unwanted variation for large-scale metabolomics data.","https://scispace.com/papers/a-hierarchical-approach-to-removal-of-unwanted-variation-for-21h5j315gv","2021","Journal Article","Nature Communications","Taiyun Kim
Taiyun Kim
Owen Tang
Stephen T Vernon
Katharine A Kott
Yen Chin Koay
Yen Chin Koay
John W. Park
David E. James
Stuart M. Grieve
Stuart M. Grieve
Terence P. Speed
Terence P. Speed
Pengyi Yang
Gemma A. Figtree
John F. O'Sullivan
Jean Yee Hwa Yang","10.1038/S41467-021-25210-5","https://scispace.com/pdf/a-hierarchical-approach-to-removal-of-unwanted-variation-for-21h5j315gv.pdf","Yes","28","Liquid chromatography-mass spectrometry-based metabolomics studies are increasingly applied to large population cohorts, which run for several weeks or even years in data acquisition. This inevitably introduces unwanted intra- and inter-batch variations over time that can overshadow true biological signals and thus hinder potential biological discoveries. To date, normalisation approaches have struggled to mitigate the variability introduced by technical factors whilst preserving biological variance, especially for protracted acquisitions. Here, we propose a study design framework with an arrangement for embedding biological sample replicates to quantify variance within and between batches and a workflow that uses these replicates to remove unwanted variation in a hierarchical manner (hRUV). We use this design to produce a dataset of more than 1000 human plasma samples run over an extended period of time. We demonstrate significant improvement of hRUV over existing methods in preserving biological signals whilst removing unwanted variation for large scale metabolomics studies. Our tools not only provide a strategy for large scale data normalisation, but also provides guidance on the design strategy for large omics studies.","In this article, the authors propose a study design framework with an arrangement for embedding biological sample replicates to quantify variance within and between batches and a workflow that uses these replicate to remove unwanted variation in a hierarchical manner (hRUV)."
"A novel bioinformatics approach to identify the consistently well-performing normalization strategy for current metabolomic studies.","https://scispace.com/papers/a-novel-bioinformatics-approach-to-identify-the-consistently-4te5juop9q","2020","Journal Article","Briefings in Bioinformatics","Qingxia Yang
Jiajun Hong
Yi Li
Weiwei Xue
Song Li
Hui Yang
Feng Zhu","10.1093/BIB/BBZ137","https://scispace.com/pdf/a-novel-bioinformatics-approach-to-identify-the-consistently-4te5juop9q.pdf","Yes","51","Unwanted experimental/biological variation and technical error are frequently encountered in current metabolomics, which requires the employment of normalization methods for removing undesired data fluctuations. To ensure the 'thorough' removal of unwanted variations, the collective consideration of multiple criteria ('intragroup variation', 'marker stability' and 'classification capability') was essential. However, due to the limited number of available normalization methods, it is extremely challenging to discover the appropriate one that can meet all these criteria. Herein, a novel approach was proposed to discover the normalization strategies that are consistently well performing (CWP) under all criteria. Based on various benchmarks, all normalization methods popular in current metabolomics were 'first' discovered to be non-CWP. 'Then', 21 new strategies that combined the 'sample'-based method with the 'metabolite'-based one were found to be CWP. 'Finally', a variety of currently available methods (such as cubic splines, range scaling, level scaling, EigenMS, cyclic loess and mean) were identified to be CWP when combining with other normalization. In conclusion, this study not only discovered several strategies that performed consistently well under all criteria, but also proposed a novel approach that could ensure the identification of CWP strategies for future biological problems.","This study discovered several strategies that performed consistently well under all criteria, but also proposed a novel approach that could ensure the identification of CWP strategies for future biological problems."
"Data pre-processing for analyzing microbiome data – A mini review","https://scispace.com/papers/data-pre-processing-for-analyzing-microbiome-data-a-mini-27yf2nnmfz","2023","","Computational and structural biotechnology journal","Ruwen Zhou
Siu Kin Ng
Joseph J.Y. Sung
Wilson Wen Bin Goh
S. H. Wong","10.1016/j.csbj.2023.10.001","","No","","The human microbiome is an emerging research frontier due to its profound impacts on health. High-throughput microbiome sequencing enables studying microbial communities but suffers from analytical challenges. In particular, the lack of dedicated preprocessing methods to improve data quality impedes effective minimization of biases prior to downstream analysis. This review aims to address this gap by providing a comprehensive overview of preprocessing techniques relevant to microbiome research. We outline a typical workflow for microbiome data analysis. Preprocessing methods discussed include quality filtering, batch effect correction, imputation of missing values, normalization, and data transformation. We highlight strengths and limitations of each technique to serve as a practical guide for researchers and identify areas needing further methodological development. Establishing robust, standardized preprocessing will be essential for drawing valid biological conclusions from microbiome studies.","Preprocessing methods discussed include quality filtering, batch effect correction, imputation of missing values, normalization, and data transformation, which will be essential for drawing valid biological conclusions from microbiome studies."
"hRUV: Hierarchical approach to removal of unwanted variation for large-scale metabolomics data","https://scispace.com/papers/hruv-hierarchical-approach-to-removal-of-unwanted-variation-1tvc621ptl","2020","Posted Content","bioRxiv","Taiyun Kim
Taiyun Kim
Owen Tang
Stephen T Vernon
Katharine A Kott
Yen Chin Koay
John W. Park
David E. James
Terence P. Speed
Terence P. Speed
Pengyi Yang
John F. O'Sullivan
Gemma A. Figtree
Jean Yee Hwa Yang","10.1101/2020.12.21.423723","https://scispace.com/pdf/hruv-hierarchical-approach-to-removal-of-unwanted-variation-1tvc621ptl.pdf","Yes","","Liquid chromatography-mass spectrometry based metabolomics studies are increasingly applied to large population cohorts, running for several weeks to months, even extending to years of data acquisition. This inevitably introduces unwanted intra- and inter-batch variations over time that can overshadow true biological signals and thus hinder potential biological discoveries. To date, normalization approaches have struggled to mitigate the variability introduced by technical factors whilst preserving biological variance, especially for protracted acquisitions. Here, we designed an experiment with an arrangement to embed biological sample replicates to measure the variance within and between batches for over 1,000 human plasma samples run over 44 days. We integrate these replicates in a novel workflow to remove unwanted variation in a hierarchical structure (hRUV) by progressively merging the adjustments in neighbouring batches. We demonstrate significant improvement of hRUV over existing methods in maintaining biological signals whilst removing unwanted variation for large scale metabolomics studies.","In this paper, the authors designed an experiment with an arrangement to embed biological sample replicates to measure the variance within and between batches for over 1,000 human plasma samples run over 44 days, and integrate these replicates in a novel workflow to remove unwanted variation in a hierarchical structure (hRUV) by progressively merging the adjustments in neighbouring batches."
"mzLearn, a data-driven LC/MS signal detection algorithm, enables pre-trained generative models for untargeted metabolomics","https://scispace.com/papers/mzlearn-a-data-driven-lc-ms-signal-detection-algorithm-5z4zwm1yn13v","2025","Journal Article","","Leila Pirhaji
Jonah Eaton
Adarsh K. Jeewajee
Min Zhang
Matthew Morris
Maria Karasarides","10.1101/2025.01.26.634927","","No","","Metabolite alterations are linked to diseases, yet large-scale untargeted metabolomics remains constrained by challenges in signal detection and integration of diverse datasets for developing pre-trained metabolic models. Here, we introduce mzLearn, a data-driven algorithm for untargeted metabolomic signal detection that requires no input parameters or metabolomics expertise. mzLearn outperforms existing methods in both signal quantity and quality, effectively correcting instrument drifts across large cohorts without experimental QC samples. mzLearn detected robust metabolite signals from 20,548 blood samples from diverse studies, enabling the development of first-in-class pre-trained generative models for untargeted metabolomics. Learned metabolite representations reflected demographic data and when fine-tuned on unseen renal cell carcinoma data, improved downstream clinical and survival tasks. Metabolite representation further facilitated joint and adversarial learning for prognostic and predictive patient stratification, outperforming clinical-grade risk criteria in survival prediction. mzLearn democratizes untargeted metabolomics data and paves the way for developing foundation models for untargeted metabolomics.","Researchers introduce mzLearn, a data-driven algorithm for untargeted metabolomics signal detection, outperforming existing methods and enabling pre-trained generative models for metabolomics, improving clinical and survival tasks, and facilitating patient stratification."
"MAFFIN: metabolomics sample normalization using maximal density fold change with high-quality metabolic features and corrected signal intensities","https://scispace.com/papers/maffin-metabolomics-sample-normalization-using-maximal-2n2be7bh","2022","Journal Article","Bioinformatics","Tao Huan","10.1093/bioinformatics/btac355","https://scispace.com/pdf/maffin-metabolomics-sample-normalization-using-maximal-2n2be7bh.pdf","Yes","","Abstract Motivation Post-acquisition sample normalization is a critical step in comparative metabolomics to remove the variation introduced by sample amount or concentration difference. Previously reported approaches are either specific to one sample type or built on strong assumptions on data structure, which are limited to certain levels. This encouraged us to develop MAFFIN, an accurate and robust post-acquisition sample normalization workflow that works universally for metabolomics data collected on mass spectrometry (MS) platforms. Results MAFFIN calculates normalization factors using maximal density fold change (MDFC) computed by a kernel density-based approach. Using both simulated data and 20 metabolomics datasets, we showcased that MDFC outperforms four commonly used normalization methods in terms of reducing the intragroup variation among samples. Two essential steps, overlooked in conventional methods, were also examined and incorporated into MAFFIN. (i) MAFFIN uses multiple orthogonal criteria to select high-quality features for normalization factor calculation, which minimizes the bias caused by abiotic features or metabolites with poor quantitative performance. (ii) MAFFIN corrects the MS signal intensities of high-quality features using serial quality control samples, which guarantees the accuracy of fold change calculations. MAFFIN was applied to a human saliva metabolomics study and led to better data separation in principal component analysis and more confirmed significantly altered metabolites. Availability and implementation The MAFFIN algorithm was implemented in an R package named MAFFIN. Package installation, user instruction and demo data are available at https://github.com/HuanLab/MAFFIN. Other data in this work are available on request. Supplementary information Supplementary data are available at Bioinformatics online.","Huan et al. as mentioned in this paper proposed an accurate and robust post-acquisition sample normalization workflow that works universally for metabolomics data collected on mass spectrometry (MS) platforms."
"DBnorm as an R package for the comparison and selection of appropriate statistical methods for batch effect correction in metabolomic studies.","https://scispace.com/papers/dbnorm-as-an-r-package-for-the-comparison-and-selection-of-174t47bhk3","2021","Journal Article","Scientific Reports","Nasim Bararpour
Federica Gilardi
Cristian Carmeli
Cristian Carmeli
Jonathan Sidibé
Julijana Ivanisevic
Tiziana Caputo
Marc Augsburger
Silke Grabherr
Béatrice Desvergne
Nicolas Guex
Murielle Bochud
Aurélien Thomas","10.1038/S41598-021-84824-3","https://serval.unil.ch/resource/serval:BIB_C00E7D1EAE17.P001/REF.pdf","Yes","","As a powerful phenotyping technology, metabolomics provides new opportunities in biomarker discovery through metabolome-wide association studies (MWAS) and the identification of metabolites having a regulatory effect in various biological processes. While mass spectrometry-based (MS) metabolomics assays are endowed with high throughput and sensitivity, MWAS are doomed to long-term data acquisition generating an overtime-analytical signal drift that can hinder the uncovering of real biologically relevant changes. We developed ""dbnorm"", a package in the R environment, which allows for an easy comparison of the model performance of advanced statistical tools commonly used in metabolomics to remove batch effects from large metabolomics datasets. ""dbnorm"" integrates advanced statistical tools to inspect the dataset structure not only at the macroscopic (sample batches) scale, but also at the microscopic (metabolic features) level. To compare the model performance on data correction, ""dbnorm"" assigns a score that help users identify the best fitting model for each dataset. In this study, we applied ""dbnorm"" to two large-scale metabolomics datasets as a proof of concept. We demonstrate that ""dbnorm"" allows for the accurate selection of the most appropriate statistical tool to efficiently remove the overtime signal drift and to focus on the relevant biological components of complex datasets.","Dbnorm as mentioned in this paper integrates advanced statistical tools to inspect the dataset structure not only at the macroscopic (sample batches) scale, but also at the microscopic (metabolic features) level."
"tsrobprep -- an R package for robust preprocessing of time series data","https://scispace.com/papers/tsrobprep-an-r-package-for-robust-preprocessing-of-time-frbrqpylxs","2021","Journal Article","arXiv: Machine Learning","Michał Narajewski
Jens Kley-Holsteg
Florian Ziel","10.1016/J.SOFTX.2021.100809","https://arxiv.org/pdf/2104.12657.pdf","Yes","","Data cleaning is a crucial part of every data analysis exercise. Yet, the currently available R packages do not provide fast and robust methods for cleaning and preparation of time series data. The open source package tsrobprep introduces efficient methods for handling missing values and outliers using model based approaches. For data imputation a probabilistic replacement model is proposed, which may consist of autoregressive components and external inputs. For outlier detection a clustering algorithm based on finite mixture modelling is introduced, which considers typical time series related properties as features. By assigning to each observation a probability of being an outlying data point, the degree of outlyingness can be determined. The methods work robust and are fully tunable. Moreover, by providing the auto_data_cleaning function the data preprocessing can be carried out in one cast, without manual tuning and providing suitable results. The primary motivation of the package is the preprocessing of energy system data, however, the package is also suited for other moderate and large sized time series data set. We present application for electricity load, wind and solar power data.","Tsrobprep as mentioned in this paper is a R package for handling missing values and outliers using model-based approaches, which can be used for energy system data, however, the package is also suited for other moderate and large sized time series data set."
"Omics Untargeted Key Script: R-Based Software Toolbox for Untargeted Metabolomics with Bladder Cancer Biomarkers Discovery Case Study.","https://scispace.com/papers/omics-untargeted-key-script-r-based-software-toolbox-for-4dsc3xljxx","2021","Journal Article","Journal of Proteome Research","I. V. Plyushchenko
Elizaveta S Fedorova
Natalia V Potoldykova
Konstantin A Polyakovskiy
A.I. Glukhov
A.I. Glukhov
Igor Rodin
Igor Rodin","10.1021/ACS.JPROTEOME.1C00392","","No","","Large-scale untargeted LC-MS-based metabolomic profiling is a valuable source for systems biology and biomarker discovery. Data analysis and processing are major tasks due to the high complexity of generated signals and the presence of unwanted variations. In the present study, we introduce an R-based open-source collection of scripts called OUKS (Omics Untargeted Key Script), which provides comprehensive data processing. OUKS is developed by integrating various R packages and metabolomics software tools and can be easily set up and prepared to create a custom pipeline. Novel computational features are related to quality control samples-based signal processing and are implemented by gradient boosting, tree-based, and other nonlinear regression algorithms. Bladder cancer biomarkers discovery study which is based on untargeted LC-MS profiling of urine samples is performed to demonstrate exhaustive functionality of the developed software tool. Unique examination among dozens of metabolomics-specific data curation methods was carried out at each processing step. As a result, potential biomarkers were identified, statistically validated, and described by metabolism disorders. Our study demonstrates that OUKS helps to make untargeted LC-MS metabolomic profiling with the latest computational features readily accessible in a ready-to-use unified manner to a research community.","Omics Untargeted Key Script (OUKS) as mentioned in this paper is an R-based open-source collection of scripts, which provides comprehensive data processing for large-scale untargeted LC-MS-based metabolomic profiling."
"Concepts and Software Package for Efficient Quality Control in Targeted Metabolomics Studies - MeTaQuaC","https://scispace.com/papers/concepts-and-software-package-for-efficient-quality-control-24jwg4oba7","2020","Posted Content","bioRxiv","Mathias Kuhring
Alina Eisenberger
Vanessa A. F. Schmidt
Nicolle Kränkel
David M. Leistner
Jennifer A. Kirwan
Dieter Beule","10.1101/2020.01.10.901710","https://scispace.com/pdf/concepts-and-software-package-for-efficient-quality-control-24jwg4oba7.pdf","Yes","","Targeted quantitative mass spectrometry metabolite profiling is the workhorse of metabolomics research. Robust and reproducible data is essential for confidence in analytical results and is particularly important with large-scale studies. Commercial kits are now available which use carefully calibrated and validated internal and external standards to provide such reliability. However, they are still subject to processing and technical errors in their use and should be subject to a laboratory’s routine quality assurance and quality control measures to maintain confidence in the results. We discuss important systematic and random measurement errors when using these kits and suggest measures to detect and quantify them. We demonstrate how wider analysis of the entire dataset, alongside standard analyses of quality control samples can be used to identify outliers and quantify systematic trends in order to improve downstream analysis. Finally we present the MeTaQuaC software which implements the above concepts and methods for Biocrates kits and creates a comprehensive quality control report containing rich visualization and informative scores and summary statistics. Preliminary unsupervised multivariate analysis methods are also included to provide rapid insight into study variables and groups. MeTaQuaC is provided as an open source R package under a permissive MIT license and includes detailed user documentation.","It is demonstrated how wider analysis of the entire dataset, alongside standard analyses of quality control samples can be used to identify outliers and quantify systematic trends in order to improve downstream analysis."
"Online Missing Value Imputation and Change Point Detection with the Gaussian Copula","https://scispace.com/papers/online-missing-value-imputation-and-change-point-detection-7sdwx6gh","2022","Journal Article","Proceedings of the ... AAAI Conference on Artificial Intelligence","","10.1609/aaai.v36i8.20906","https://scispace.com/pdf/online-missing-value-imputation-and-change-point-detection-7sdwx6gh.pdf","Yes","","Missing value imputation is crucial for real-world data science workflows. Imputation is harder in the online setting, as it requires the imputation method itself to be able to evolve over time. For practical applications, imputation algorithms should produce imputations that match the true data distribution, handle data of mixed types, including ordinal, boolean, and continuous variables, and scale to large datasets. In this work we develop a new online imputation algorithm for mixed data using the Gaussian copula. The online Gaussian copula model produces meets all the desiderata: its imputations match the data distribution even for mixed data, improve over its offline counterpart on the accuracy when the streaming data has a changing distribution, and on the speed (up to an order of magnitude) especially on large scale datasets. By fitting the copula model to online data, we also provide a new method to detect change points in the multivariate dependence structure for mixed data with missing values. Experimental results on synthetic and real world data validate the performance of the proposed methods.","In this article , a new online imputation algorithm for mixed data using the Gaussian copula model is developed, which can produce imputations that match the true data distribution, handle data of mixed types, including ordinal, boolean, and continuous variables, and scale to large datasets."