"Paper Title","Paper Link","Publication Year","Publication Type","Publication Title","Author Names","DOI","PDF Link","Open Access","Citations count","Abstract","TL;DR"
"High-dimensional Statistics Applications to Batch Effects in
  Metabolomics","https://scispace.com/papers/high-dimensional-statistics-applications-to-batch-effects-in-26zrdg4b47i1","2024","Journal Article","","Zhendong Guo","10.48550/arxiv.2412.10196","https://scispace.compdf/high-dimensional-statistics-applications-to-batch-effects-in-26zrdg4b47i1.pdf","No","","Batch effects are inevitable in large-scale metabolomics. Prior to formal data analysis, batch effect correction (BEC) is applied to prevent from obscuring biological variations, and batch effect evaluation (BEE) is used for correction assessment. However, existing BEE algorithms neglect covariances between the variables, and existing BEC algorithms might fail to adequately correct the covariances. Therefore, we resort to recent advancements in high-dimensional statistics, and respectively propose ""quality control-based simultaneous tests (QC-ST)"" and ""covariance correction (CoCo)"". Validated by the simulation data, QC-ST can simultaneously detect the statistical significance of QC samples' mean vectors and covariance matrices across different batches, and has a satisfactory statistical performance in empirical sizes, empirical powers, and computational speed. Then, we apply four QC-based BEC algorithms to two large cohort datasets, and find that extreme gradient boost (XGBoost) performs best in relative standard deviation (RSD) and dispersion-ratio (D-ratio). After prepositive BEC, if QC-ST still suggests that batch effects between some two batches are significant, CoCo should be implemented. And after CoCo (if necessary), the four metrics (i.e., RSD, D-ratio, classification performance, and QC-ST) might be further improved. In summary, under the guidance of QC-ST, we can develop a matching strategy to integrate multiple BEC algorithms more rationally and flexibly, and minimize batch effects for reliable biological conclusions.","This study proposes high-dimensional statistics-based methods, QC-ST and CoCo, to address batch effects in metabolomics, improving batch effect correction and evaluation by considering covariances between variables, and enhancing statistical performance and reliability of biological conclusions."
"Garantia de qualidade em metabolômica não-alvo","https://scispace.com/papers/garantia-de-qualidade-em-metabolomica-nao-alvo-58lkmbxm2wki","2025","Journal Article","Química Nova","Anderson Teixeira
João Welberthon Matos Queiroz
Bruno Carius Garrido
Antônio Jorge Ribeiro da Silva
Anelize Bauermeister
Ricardo Moreira Borges","10.21577/0100-4042.20250048","","No","","QUALITY ASSURANCE IN UNTARGETED METABOLOMICS. Over the last two decades, metabolomics has emerged as a pivotal tool in multidisciplinary research, providing invaluable insights into metabolome modulations and finding applications across various scientific domains, including medicine and agronomy. Nonetheless, the absence of standardized procedures in sample preparation, data acquisition, and documentation of data quality presents a significant challenge. This review underscores the critical importance of quality assurance and quality control (QA/QC) in untargeted metabolomics, advocating for the establishment of agreed-upon QA/QC reporting standards within the scientific community. We discuss the requisite quality controls at various stages of untargeted metabolomics studies, encompassing blank samples, pooled samples, and the utilization of external quality control samples. Methods for assessing accuracy, reproducibility, and identifying/correcting batch effects are addressed. Furthermore, emphasis is placed on standardizing the description of QA/QC data in scientific publications and repositories to foster reproducibility and transparency. We recommend the publication of QC data alongside studies in appropriate databases to facilitate data comparison and sharing among researchers, thereby enhancing the quality of untargeted metabolomics research. In summary, implementing standardized QA/QC data reporting, along with promoting best practices within the untargeted metabolomics community, is crucial for improving result quality and credibility and advancing research utilizing this powerful technique.","This review emphasizes the importance of quality assurance and control in untargeted metabolomics, advocating for standardized procedures and reporting standards to ensure accuracy, reproducibility, and transparency in metabolomics research and data sharing."
"Correcting batch effects in large-scale multiomic studies using a reference-material-based ratio method","https://scispace.com/papers/correcting-batch-effects-in-large-scale-multiomic-studies-fxcx50oa","2022","Posted Content","bioRxiv","Ying Yu
Naixin Zhang
Yuanbang Mai
Qiaochu Chen
Zehui Cao
Qingwang Chen
Yaqing Liu
Luyao Ren
Wanwan Hou
Jingcheng Yang
Huixiao Hong
Joshua Xu
Weida Tong
Leming Shi
Yuanting Zheng","10.1101/2022.10.19.507549","https://scispace.com/pdf/correcting-batch-effects-in-large-scale-multiomic-studies-fxcx50oa.pdf","Yes","","Batch effects are notorious technical variations that are common in multiomic data and may result in misleading outcomes. With the era of big data, tackling batch effects in multiomic integration is urgently needed. As part of the Quartet Project for quality control and data integration of multiomic profiling, we comprehensively assess the performances of seven batch-effect correction algorithms (BECAs) for mitigating the negative impact of batch effects in multiomic datasets, including transcriptomics, proteomics, and metabolomics. Performances are evaluated based on accuracy of identifying differentially expressed features, robustness of predictive models, and the ability of accurately clustering cross-batch samples into their biological sample groups. Ratio-based method is more effective and widely applicable than others, especially in cases when batch effects are highly confounded with biological factors of interests. We further provide practical guidelines for the implementation of ratio-based method using universal reference materials profiled with study samples. Our findings show the promise for eliminating batch effects and enhancing data integration in increasingly large-scale, cross-batch multiomic studies.","In this article , the authors comprehensively assess the performance of seven batch effect correction algorithms (BECAs) for mitigating the negative impact of batch effects in multomic datasets, including transcriptomics, proteomics, and metabolomics."
"Evaluating and minimizing batch effects in metabolomics.","https://scispace.com/papers/evaluating-and-minimizing-batch-effects-in-metabolomics-51ppw3tzoa","2020","Journal Article","Mass Spectrometry Reviews","Wei Han
Liang Li","10.1002/MAS.21672","","No","62","Determining metabolomic differences among samples of different phenotypes is a critical component of metabolomics research. With the rapid advances in analytical tools such as ultrahigh-resolution chromatography and mass spectrometry, an increasing number of metabolites can now be profiled with high quantification accuracy. The increased detectability and accuracy raise the level of stringiness required to reduce or control any experimental artifacts that can interfere with the measurement of phenotype-related metabolome changes. One of the artifacts is the batch effect that can be caused by multiple sources. In this review, we discuss the origins of batch effects, approaches to detect interbatch variations, and methods to correct unwanted data variability due to batch effects. We recognize that minimizing batch effects is currently an active research area, yet a very challenging task from both experimental and data processing perspectives. Thus, we try to be critical in describing the performance of a reported method with the hope of stimulating further studies for improving existing methods or developing new methods.","In this review, the origins of batch effects, approaches to detect interbatch variations, and methods to correct unwanted data variability due to batch effects are discussed."
"Addressing the batch effect issue for LC/MS metabolomics data in data preprocessing.","https://scispace.com/papers/addressing-the-batch-effect-issue-for-lc-ms-metabolomics-2zrz79p21i","2020","Journal Article","Scientific Reports","Qin Liu
Douglas I. Walker
Karan Uppal
Zihe Liu
Chunyu Ma
ViLinh Tran
Shuzhao Li
Dean P. Jones
Tianwei Yu","10.1038/S41598-020-70850-0","https://scispace.com/pdf/addressing-the-batch-effect-issue-for-lc-ms-metabolomics-2zrz79p21i.pdf","Yes","47","With the growth of metabolomics research, more and more studies are conducted on large numbers of samples. Due to technical limitations of the Liquid Chromatography-Mass Spectrometry (LC/MS) platform, samples often need to be processed in multiple batches. Across different batches, we often observe differences in data characteristics. In this work, we specifically focus on data generated in multiple batches on the same LC/MS machinery. Traditional preprocessing methods treat all samples as a single group. Such practice can result in errors in the alignment of peaks, which cannot be corrected by post hoc application of batch effect correction methods. In this work, we developed a new approach that address the batch effect issue in the preprocessing stage, resulting in better peak detection, alignment and quantification. It can be combined with down-stream batch effect correction methods to further correct for between-batch intensity differences. The method is implemented in the existing workflow of the apLCMS platform. Analyzing data with multiple batches, both generated from standardized quality control (QC) plasma samples and from real biological studies, the new method resulted in feature tables with better consistency, as well as better down-stream analysis results. The method can be a useful addition to the tools available for large studies involving multiple batches. The method is available as part of the apLCMS package. Download link and instructions are at https://mypage.cuhk.edu.cn/academics/yutianwei/apLCMS/ .","A new approach is developed that address the batch effect issue in the preprocessing stage, resulting in better peak detection, alignment and quantification, and can be combined with down-stream batch effect correction methods to further correct for between-batch intensity differences."
"Correcting batch effects in large-scale multiomics studies using a reference-material-based ratio method","https://scispace.com/papers/correcting-batch-effects-in-large-scale-multiomics-studies-3hcwkemp3r","2023","Dataset","","Ying Yu","10.6084/m9.figshare.22188349","","No","","As part of the Quartet Project for quality control and data integration of multiomics profiling, we comprehensively assessed the performance of seven batch-effect correction algorithms (BECAs) based on different performance metrics of clinical relevance. Fifteen batches of transcriptomics, proteomics, and metabolomics data from different platforms, labs and with different data quality were employed and referred as full datasets in this study. In the full datasets, each batch comprised 12 libraries, consisting of 12 tubes with each representing one of the triplicates of a donor (D5, D6, F7 and M8). Therefore, 180 libraries (12 libraries per batch x 15 batches) were included in full datasets at each omics level. We then employed a subset of datasets from the full datasets to create balanced and confounded scenarios for assessing the pros and cons of the BECAs. Here, we arbitrarily selected D6 as the common reference material, leaving the rest three as the study groups (D5, F7, and M8). In the balanced experiment scenario, one replicate was selected for each study group from each of 15 batches. This was done independently for each omics type. In the confounded experiment scenario, 5 batches were randomly assigned to each study group (D5, F7, or M8) for each omics type to extract all three replicates for the assigned study group. For both scenarios, all three replicates for the selected reference sample (D6) in each batch were retained for reference-sample-based BECAs. Therefore, 45 study samples and 45 reference samples in balanced and confounded scenarios were employed at each omics level. The experimental design ensured the consistent number of libraries included in the balanced and confounded scenarios, as well as the separation of study samples from the reference samples for objective evaluation of the impact of BECAs. Data analysis methods used in the study were as follows. (1) For transcriptomics, RNAseq reads were aligned using HISAT2 and genes were quantified using StringTie followed by Ballgown. The normalized data in Fragments Per Kilobase of transcript per Million mapped reads (FPKM) were obtained. A floor value of 0.01 was added to the FPKM value of each gene, and log2 transformation was then conducted. (2) For proteomics, MS raw files were searched against the human Refseq protein database using Firmiana 1.0 enabled with Mascot 2.3 (Matrix Science Inc) . False discovery rate (FDR) by using a target-decoy strategy was set to 1% for both proteins and peptides. Proteins were then quantified using the label-free iBAQ approach. The fraction-of-total (FOT) was used to represent the normalized abundance of a particular protein, which was defined as a protein’s iBAQ value divided by the total iBAQ of all identiﬁed proteins within one sample. A floor value of 0.01 was then added to the value of each protein, and log2 transformation was conducted. (3) For metabolomics, raw data were extracted, peak-identified and QC processed using the in-house methods in each lab. Compound identification was conducted using in-house library based on the retention time/index (RI), mass to charge ratio (m/z), and MS spectral data for each metabolite. Metabolite quantification was conducted using area-under-the-curve or the concentration calculated by calibration curve using standards of each metabolite. A floor value of 1 was then added to the value of each metabolite, and log2 transformation was conducted.","The study comprehensively assessed the performance of seven batch-effect correction algorithms (BECAs) based on clinical-relevance metrics using full datasets comprising 180 libraries (12 libraries per batch) from 15 batches of transcriptomics, proteomics, and metabolomics data."
"Evaluation and correction of injection order effects in LC-MS/MS based targeted metabolomics.","https://scispace.com/papers/evaluation-and-correction-of-injection-order-effects-in-lc-2lm0km8m","2022","Journal Article","Social Science Research Network","Yang Yue
Xun Bao
Jun Jiang
Jing Li","10.2139/ssrn.4191057","","No","","For large-scale and long-term metabolomics studies that involve a large batch or multiple batches of analyses, batch effects cause nonbiological systematic biases that may lead to false positive or false negative findings. Quantitative monitoring and correction of batch effects is critical to the development of reproducible and robust metabolomics platforms either for untargeted or targeted analyses. To achieve sufficient retention and separation of a broad range of metabolites with diverse chemical structures and physicochemical properties, LC-MS/MS based targeted metabolomics often involves 3 complemented chromatographic separation methods, including reversed-phase liquid chromatography (RP-LC), hydrophilic interaction liquid chromatography (HILIC), and ion-pair liquid chromatography (IP-LC). The purpose of this study is to quantitatively evaluate intra-batch variations or injection order effects of the RP-LC, HILIC, and IP-LC methods for targeted metabolomics analyses, and develop strategies to minimize intra-batch variations and correct injection order effects for problematic metabolites. Both RP-LC and HILIC methods exhibit robust intra-batch reproducibility in 0.2 µM standard mix QC, with ∼96 % of the measured metabolites showing acceptable intra-batch variations (<20 %); whereas, the intra-batch reproducibility for some metabolites in cell matrix QC may be compromised due to stability issue, suboptimal chromatographic retention, and/or matrix effects causing ionization suppression and/or retention instability. The IP-LC method exhibits significant injection order effects, which could be effectively corrected by the developed exponential models of signal drift trends as a function of injection order for individual targeted metabolites.","The purpose of this study is to quantitatively evaluate intra-batch variations or injection order effects of the RP-LC, HILIC, and IP-LC methods for targeted metabolomics analyses, and develop strategies to minimize intra- batch variations and correctjection order effects for problematic metabolites."
"Assessing and mitigating batch effects in large-scale omics studies","https://scispace.com/papers/assessing-and-mitigating-batch-effects-in-large-scale-omics-2l4ixmfixhh3","2024","Journal Article","Genome Biology","Ying Yu
Wei Ma
Yuanting Zheng
Leming Shi","10.1186/s13059-024-03401-9","","No","","Batch effects in omics data are notoriously common technical variations unrelated to study objectives, and may result in misleading outcomes if uncorrected, or hinder biomedical discovery if over-corrected. Assessing and mitigating batch effects is crucial for ensuring the reliability and reproducibility of omics data and minimizing the impact of technical variations on biological interpretation. In this review, we highlight the profound negative impact of batch effects and the urgent need to address this challenging problem in large-scale omics studies. We summarize potential sources of batch effects, current progress in evaluating and correcting them, and consortium efforts aiming to tackle them.","Batch effects in omics data can lead to misleading outcomes if uncorrected or hinder discovery if over-corrected, necessitating assessment and mitigation to ensure reliability and reproducibility of large-scale omics studies and minimize technical variation impact."
"Correcting batch effects in large-scale multiomics studies using a reference-material-based ratio method","https://scispace.com/papers/correcting-batch-effects-in-large-scale-multiomics-studies-1adpp2nhq9","2023","Journal Article","Genome Biology","Ying Yu
Naixin Zhang
Yuanbang Mai
Luyao Ren
Qiaochu Chen
Zehui Cao
Yaqing Liu
Wanwan Hou
Jingcheng Yang
Huixiao Hong
Joshua Xu
Weida Tong
Lianhua Dong
Leming Shi
Xiang Fang
Yuanting Zheng","10.1186/s13059-023-03047-z","","No","21","Batch effects are notoriously common technical variations in multiomics data and may result in misleading outcomes if uncorrected or over-corrected. A plethora of batch-effect correction algorithms are proposed to facilitate data integration. However, their respective advantages and limitations are not adequately assessed in terms of omics types, the performance metrics, and the application scenarios.As part of the Quartet Project for quality control and data integration of multiomics profiling, we comprehensively assess the performance of seven batch effect correction algorithms based on different performance metrics of clinical relevance, i.e., the accuracy of identifying differentially expressed features, the robustness of predictive models, and the ability of accurately clustering cross-batch samples into their own donors. The ratio-based method, i.e., by scaling absolute feature values of study samples relative to those of concurrently profiled reference material(s), is found to be much more effective and broadly applicable than others, especially when batch effects are completely confounded with biological factors of study interests. We further provide practical guidelines for implementing the ratio based approach in increasingly large-scale multiomics studies.Multiomics measurements are prone to batch effects, which can be effectively corrected using ratio-based scaling of the multiomics data. Our study lays the foundation for eliminating batch effects at a ratio scale.","Multiomics measurements are prone to batch effects, which can be effectively corrected using ratio-based scaling of the multiomics data, and this study lays the foundation for eliminating batch effects at a ratio scale."
"Concordance-Based Batch Effect Correction for Large-Scale Metabolomics.","https://scispace.com/papers/concordance-based-batch-effect-correction-for-large-scale-8c7ji3o0","2023","Journal Article","Analytical Chemistry","Fanjing Guo
Liheng Dong
Kian Kai Cheng
Lingli Deng
Xiangang Xu
Daniel Raftery
Jiyang Dong","10.1021/acs.analchem.2c05748","","No","","For a large-scale metabolomics study, sample collection, preparation, and analysis may last several days, months, or even (intermittently) over years. This may lead to apparent batch effects in the acquired metabolomics data due to variability in instrument status, environmental conditions, or experimental operators. Batch effects may confound the true biological relationships among metabolites and thus obscure real metabolic changes. At present, most of the commonly used batch effect correction (BEC) methods are based on quality control (QC) samples, which require sufficient and stable QC samples. However, the quality of the QC samples may deteriorate if the experiment lasts for a long time. Alternatively, isotope-labeled internal standards have been used, but they generally do not provide good coverage of the metabolome. On the other hand, BEC can also be conducted through a data-driven method, in which no QC sample is needed. Here, we propose a novel data-driven BEC method, namely, CordBat, to achieve concordance between each batch of samples. In the proposed CordBat method, a reference batch is first selected from all batches of data, and the remaining batches are referred to as ""other batches."" The reference batch serves as the baseline for the batch adjustment by providing a coordinate of correlation between metabolites. Next, a Gaussian graphical model is built on the combined dataset of reference and other batches, and finally, BEC is achieved by optimizing the correction coefficients in the other batches so that the correlation between metabolites of each batch and their combinations are in concordance with that of the reference batch. Three real-world metabolomics datasets are used to evaluate the performance of CordBat by comparing it with five commonly used BEC methods. The present experimental results showed the effectiveness of CordBat in batch effect removal and the concordance of correlation between metabolites after BEC. CordBat was found to be comparable to the QC-based methods and achieved better performance in the preservation of biological effects. The proposed CordBat method may serve as an alternative BEC method for large-scale metabolomics that lack proper QC samples.","CordBat as discussed by the authors is a data-driven batch effect correction method for large-scale metabolomics that lacks proper quality control (QC) samples, which may lead to apparent batch effects due to variability in instrument status, environmental conditions, or experimental operators."
"mzQuality: A tool for quality monitoring and reporting of targeted mass spectrometry measurements","https://scispace.com/papers/mzquality-a-tool-for-quality-monitoring-and-reporting-of-7d4vlwqyrmq6","2025","Journal Article","","Marielle van der Peet
Pascal Maas
Agnieszka B. Wegrzyn
Lieke Lamont
Ronan M. T. Fleming
Amy C. Harms
Thomas Hankemeier
Alida Kindt","10.1101/2025.01.22.633547","","No","","Analyzing metabolites using mass spectrometry can offer valuable insight into an individual's health or disease status. However, various sources of experimental variation can affect the data, making robust quality control essential. In this context, we introduce mzQuality, a user-friendly software tool designed to evaluate and correct technical variations in mass spectrometry-based metabolomics data. MzQuality offers key quality control features, such as batch correction, outlier identification, and analysis of signal-to-noise ratios. It supports any peak-integrated processed data independent of vendor software and does not require the user to have any programming skills. We demonstrate the functionality of mzQuality with a data set of 419 samples measured across six batches, in which mzQuality effectively minimized experimental variation, ensuring the data's readiness for statistical analysis and biological interpretation. With customizable settings, mzQuality can be seamlessly integrated into research workflows to produce more accurate and reproducible metabolomics data.","mzQuality is a user-friendly software tool for quality monitoring and reporting of targeted mass spectrometry measurements, offering batch correction, outlier identification, and signal-to-noise ratio analysis, ensuring accurate and reproducible metabolomics data."
"malbacR: A Package for Standardized Implementation of Batch Correction Methods for Omics Data.","https://scispace.com/papers/malbacr-a-package-for-standardized-implementation-of-batch-22st20wvas","2023","Journal Article","Analytical Chemistry","Damon T Leach
Kelly G. Stratton
Jan Irvahn
Rachel Richardson
Bobbie-Jo M. Webb-Robertson
Lisa M. Bramer","10.1021/acs.analchem.3c01289","","No","","Mass spectrometry is a powerful tool for identifying and analyzing biomolecules such as metabolites and lipids in complex biological samples. Liquid chromatography and gas chromatography mass spectrometry studies quite commonly involve large numbers of samples, which can require significant time for sample preparation and analyses. To accommodate such studies, the samples are commonly split into batches. Inevitably, variations in sample handling, temperature fluctuation, imprecise timing, column degradation, and other factors result in systematic errors or biases of the measured abundances between the batches. Numerous methods are available via R packages to assist with batch correction for omics data; however, since these methods were developed by different research teams, the algorithms are available in separate R packages, each with different data input and output formats. We introduce the malbacR package, which consolidates 11 common batch effect correction methods for omics data into one place so users can easily implement and compare the following: pareto scaling, power scaling, range scaling, ComBat, EigenMS, NOMIS, RUV-random, QC-RLSC, WaveICA2.0, TIGER, and SERRF. The malbacR package standardizes data input and output formats across these batch correction methods. The package works in conjunction with the pmartR package, allowing users to seamlessly include the batch effect correction in a pmartR workflow without needing any additional data manipulation.","The malbacR package is introduced, which consolidates 11 common batch effect correction methods for omics data into one place so users can easily implement and compare the following: pareto scaling, power scaling, range scaling, ComBat, EigenMS, NOMIS, RUV-random, QC-RLSC, WaveICA2.0, TIGER, and SERRF."
"Norm ISWSVR: A Data Integration and Normalization Approach for Large-Scale Metabolomics.","https://scispace.com/papers/norm-iswsvr-a-data-integration-and-normalization-approach-13ofbbyl","2022","Journal Article","Analytical Chemistry","Xian Ding
Fen Yang
Yanhua Chen
Jing Xu
Jiuming He
Rui Zhang
Zeper Abliz","10.1021/acs.analchem.1c05502","","No","","Large-scale and long-period metabolomics study is more susceptible to various sources of systematic errors, resulting in nonreproducibility and poor data quality. A reliable and robust batch correction method removes unwanted systematic variations and improves the statistical power of metabolomics data, which undeniably becomes an important issue for the quality control of metabolomics. This study proposed a novel data normalization and integration method, Norm ISWSVR. It is a two-step approach via combining the best-performance internal standard correction with support vector regression normalization, comprehensively removing the systematic and random errors and matrix effects. This method was investigated in three untargeted lipidomics or metabolomics datasets, and the performance was further evaluated systematically in comparison with that of 11 other normalization methods. As a result, Norm ISWSVR decreased the data's median cross-validated relative standard deviation (cvRSD), increased the correlation between QCs, improved the classification accuracy of biomarkers, and was well-compatible with quantitative data. More importantly, Norm ISWSVR also allows a low frequency of QCs, which could significantly decrease the burden of a large-scale experiment. Correspondingly, Norm ISWSVR favorably improves the data quality of large-scale metabolomics data.","This study proposed a novel data normalization and integration method, Norm ISWSVR, which is a two-step approach via combining the best-performance internal standard correction with support vector regression normalization, comprehensively removing the systematic and random errors and matrix effects."
"WiNNbeta: Batch and drift correction method by white noise normalization
  for metabolomic studies","https://scispace.com/papers/winnbeta-batch-and-drift-correction-method-by-white-noise-s3af0jvbzd","2024","Preprint","","Olga Demler
Franco Giulianini
Yanyan Liu
Malte Londschien
Anja Sjöström
Tanmay Tanna
Heike Luttmann‐Gibson
Antoine Jeanrenaud","10.48550/arxiv.2404.07906","https://scispace.compdf/winnbeta-batch-and-drift-correction-method-by-white-noise-s3af0jvbzd.pdf","No","","We developed a method called batch and drift correction method by White Noise Normalization (WiNNbeta) to correct individual metabolites for batch effects and drifts. This method tests for white noise properties to identify metabolites in need of correction and corrects them by using fine-tuned splines. To test the method performance we applied WiNNbeta to LC-MS data from our metabolomic studies and computed CVs before and after WiNNbeta correction in quality control samples.","WiNNbeta is a method for correcting batch and drift effects in metabolomic studies using white noise normalization and spline correction."
"Review of Batch Effects Prevention, Diagnostics, and Correction Approaches.","https://scispace.com/papers/review-of-batch-effects-prevention-diagnostics-and-2i256j6xw2","2020","Book Chapter","Methods of Molecular Biology","Jelena Čuklina
Patrick G. A. Pedrioli
Ruedi Aebersold","10.1007/978-1-4939-9744-2_16","","No","38","Systematic technical variation in high-throughput studies consisting of the serial measurement of large sample cohorts is known as batch effects. Batch effects reduce the sensitivity of biological signal extraction and can cause significant artifacts. The systematic bias in the data caused by batch effects is more common in studies in which logistical considerations restrict the number of samples that can be prepared or profiled in a single experiment, thus necessitating the arrangement of subsets of study samples in batches. To mitigate the negative impact of batch effects, statistical approaches for batch correction are used at the stage of experimental design and data processing. Whereas in genomics batch effects and possible remedies have been extensively discussed, they are a relatively new challenge in proteomics because methods with sufficient throughput to systematically measure through large sample cohorts have only recently become available. Here we provide general recommendations to mitigate batch effects: we discuss the design of large-scale proteomic studies, review the most commonly used tools for batch effect correction and overview their application in proteomics.","The design of large-scale proteomic studies is discussed, the most commonly used tools for batch effect correction are reviewed, their application in proteomics is overview and general recommendations to mitigate batch effects are provided."
"Instrumental Drift in Untargeted Metabolomics: Optimizing Data Quality with Intrastudy QC Samples","https://scispace.com/papers/instrumental-drift-in-untargeted-metabolomics-optimizing-626njri1","2023","Journal Article","Metabolites","Andre Märtens
Johannes Holle
Brit Mollenhauer
Andre Wegner
Jennifer A. Kirwan
Karsten Hiller","10.3390/metabo13050665","https://www.mdpi.com/2218-1989/13/5/665/pdf?version=1684232827","Yes","13","Untargeted metabolomics is an important tool in studying health and disease and is employed in fields such as biomarker discovery and drug development, as well as precision medicine. Although significant technical advances were made in the field of mass-spectrometry driven metabolomics, instrumental drifts, such as fluctuations in retention time and signal intensity, remain a challenge, particularly in large untargeted metabolomics studies. Therefore, it is crucial to consider these variations during data processing to ensure high-quality data. Here, we will provide recommendations for an optimal data processing workflow using intrastudy quality control (QC) samples that identifies errors resulting from instrumental drifts, such as shifts in retention time and metabolite intensities. Furthermore, we provide an in-depth comparison of the performance of three popular batch-effect correction methods of different complexity. By using different evaluation metrics based on QC samples and a machine learning approach based on biological samples, the performance of the batch-effect correction methods were evaluated. Here, the method TIGER demonstrated the overall best performance by reducing the relative standard deviation of the QCs and dispersion-ratio the most, as well as demonstrating the highest area under the receiver operating characteristic with three different probabilistic classifiers (Logistic regression, Random Forest, and Support Vector Machine). In summary, our recommendations will help to generate high-quality data that are suitable for further downstream processing, leading to more accurate and meaningful insights into the underlying biological processes.","In this paper , the authors provide recommendations for an optimal data processing workflow using intrastudy quality control (QC) samples that identifies errors resulting from instrumental drifts, such as shifts in retention time and metabolite intensities."
"WaveICA 2.0: a novel batch effect removal method for untargeted metabolomics data without using batch information.","https://scispace.com/papers/waveica-2-0-a-novel-batch-effect-removal-method-for-lwf92ya4o5","2021","Journal Article","Metabolomics","Kui Deng
Falin Zhao
Zhiwei Rong
Lei Cao
Liuchao Zhang
Kang Li
Yan Hou
Zheng-Jiang Zhu","10.1007/S11306-021-01839-7","","No","","Untargeted metabolomics based on liquid chromatography-mass spectrometry is inevitably affected by batch effects that are caused by non-biological systematic bias. Previously, we developed a novel method called WaveICA to remove batch effects for untargeted metabolomics data. To detect batch effect information, the method relies on a batch label. However, it cannot be used in the scenario in which there is only one batch of data or the batch label is unknown. We aim to improve the WaveICA method to remove batch effects for untargeted metabolomics data without using batch information. We improved the WaveICA method by developing WaveICA 2.0 to remove batch effects for metabolomics data, and provided an R package WaveICA_2.0 to implement this method. The performance of the WaveICA 2.0 method was evaluated on real metabolomics data. For metabolomics data with three batches, the performance of the WaveICA 2.0 method was similar to that of the WaveICA method in terms of gathering quality control samples (QCSs) and subject samples together in principle component analysis score plots, increasing the similarity of QCSs, increasing differential peaks, and improving classification accuracy. For metabolomics data with only one batch, the WaveICA 2.0 method had a strong ability to remove intensity drift and reveal more biological information and outperformed the QC-RLSC and QC-SVRC methods in our study using our metabolomics data. Our results demonstrated that the WaveICA 2.0 method can be used in practice to remove batch effects for untargeted metabolomics data without batch information.","In this article, the WaveICA 2.0 method was proposed to remove batch effects for untargeted metabolomics data without using batch information, which can be used in practice to remove intensity drift and reveal more biological information."
"QComics: Recommendations and Guidelines for Robust, Easily Implementable and Reportable Quality Control of Metabolomics Data.","https://scispace.com/papers/qcomics-recommendations-and-guidelines-for-robust-easily-q771w8v1v5","2024","Journal Article","Analytical Chemistry","Alvaro González-Dominguez
Núria Estanyol-Torres
Carl Brunius
Rikard Landberg
Raúl González-Domínguez","10.1021/acs.analchem.3c03660","","No","","The implementation of quality control strategies is crucial to ensure the reproducibility, accuracy, and meaningfulness of metabolomics data. However, this pivotal step is often overlooked within the metabolomics workflow and frequently relies on the use of nonstandardized and poorly reported protocols. To address current limitations in this respect, we have developed QComics, a robust, easily implementable and reportable method for monitoring and controlling data quality. The protocol operates in various sequential steps aimed to (i) correct for background noise and carryover, (ii) detect signal drifts and ""out-of-control"" observations, (iii) deal with missing data, (iv) remove outliers, (v) monitor quality markers to identify samples affected by improper collection, preprocessing, or storage, and (vi) assess overall data quality in terms of precision and accuracy. Notably, this tool considers important issues often neglected along quality control, such as the need of separately handling missing values and truly absent data to avoid losing relevant biological information, as well as the large impact that preanalytical factors may elicit on metabolomics results. Altogether, the guidelines compiled in QComics might contribute to establishing gold standard recommendations and best practices for quality control within the metabolomics community.","QComics provides robust, easily implementable and reportable guidelines for quality control of metabolomics data, ensuring reproducibility, accuracy and meaningfulness."
"metabolomicsR: a streamlined workflow to analyze metabolomic data in R","https://scispace.com/papers/metabolomicsr-a-streamlined-workflow-to-analyze-metabolomic-3oy1vouo","2022","Journal Article","Bioinformatics advances","Xikun Han
Liming Liang","10.1093/bioadv/vbac067","https://scispace.com/pdf/metabolomicsr-a-streamlined-workflow-to-analyze-metabolomic-3oy1vouo.pdf","Yes","","Abstract Summary metabolomicsR is a streamlined, flexible and user-friendly R package to preprocess, analyze and visualize metabolomic data. metabolomicsR includes comprehensive functionalities for sample and metabolite quality control, outlier detection, missing value imputation, dimensional reduction, batch effect normalization, data integration, regression, metabolite annotation and visualization of data and results. In this application note, we demonstrate the step-by-step use of the main functions from this package. Availability and implementation The metabolomicsR package is available via CRAN and GitHub (https://github.com/XikunHan/metabolomicsR/). A step-by-step online tutorial is available at https://xikunhan.github.io/metabolomicsR/docs/articles/Introduction.html. Supplementary information Supplementary data are available at Bioinformatics Advances online.","A streamlined, flexible and user-friendly R package to preprocess, analyze and visualize metabolomic data that includes comprehensive functionalities for sample and metabolite quality control, outlier detection, missing value imputation, dimensional reduction, batch effect normalization, data integration, regression, metabolite annotation and visualization of data and results."
"Batch effect correction with sample remeasurement in highly confounded case-control studies","https://scispace.com/papers/batch-effect-correction-with-sample-remeasurement-in-highly-2n0inz622t","2023","Preprint","","Hanxuan Ye
Xianyang Zhang
Chen Wang
Ellen L. Goode
Jun Chen","10.48550/arxiv.2311.03289","","No","","Batch effects are pervasive in biomedical studies. One approach to address the batch effects is repeatedly measuring a subset of samples in each batch. These remeasured samples are used to estimate and correct the batch effects. However, rigorous statistical methods for batch effect correction with remeasured samples are severely under-developed. In this study, we developed a framework for batch effect correction using remeasured samples in highly confounded case-control studies. We provided theoretical analyses of the proposed procedure, evaluated its power characteristics, and provided a power calculation tool to aid in the study design. We found that the number of samples that need to be remeasured depends strongly on the between-batch correlation. When the correlation is high, remeasuring a small subset of samples is possible to rescue most of the power.","Batch effect correction with remeasured samples in highly confounded case-control studies is a well-powered approach to mitigate batch effects. The number of samples that need to be remeasured depends on the between-batch correlation."
"Quality Control of Targeted Plasma Lipids in a Large-Scale Cohort Study Using Liquid Chromatography–Tandem Mass Spectrometry","https://scispace.com/papers/quality-control-of-targeted-plasma-lipids-in-a-large-scale-1kddg0xj","2023","Journal Article","Metabolites","Akiyoshi Hirayama
Takamasa Ishikawa
Sanae Yamanaka
Satsuki Ikeda
Aya Hirata
Seizaburo Harada
Masahiro Sugimoto
Tomoyoshi Soga
Masaru Tomita
Toru Takebayashi","10.3390/metabo13040558","https://www.mdpi.com/2218-1989/13/4/558/pdf?version=1681397925","Yes","","High-throughput metabolomics has enabled the development of large-scale cohort studies. Long-term studies require multiple batch-based measurements, which require sophisticated quality control (QC) to eliminate unexpected bias to obtain biologically meaningful quantified metabolomic profiles. Liquid chromatography–mass spectrometry was used to analyze 10,833 samples in 279 batch measurements. The quantified profile included 147 lipids including acylcarnitine, fatty acids, glucosylceramide, lactosylceramide, lysophosphatidic acid, and progesterone. Each batch included 40 samples, and 5 QC samples were measured for 10 samples of each. The quantified data from the QC samples were used to normalize the quantified profiles of the sample data. The intra- and inter-batch median coefficients of variation (CV) among the 147 lipids were 44.3% and 20.8%, respectively. After normalization, the CV values decreased by 42.0% and 14.7%, respectively. The effect of this normalization on the subsequent analyses was also evaluated. The demonstrated analyses will contribute to obtaining unbiased, quantified data for large-scale metabolomics.","In this paper , the authors used liquid chromatography-mass spectrometry (LCS) to analyze 10,833 samples in 279 batch measurements and quantified profile included 147 lipids including acylcarnitine, fatty acids, glucosylceramide, lacto-ceramide and progesterone."
"An anchored experimental design and meta-analysis approach to address batch effects in large-scale metabolomics","https://scispace.com/papers/an-anchored-experimental-design-and-meta-analysis-approach-1r7zj63y","2022","Posted Content","","Cuiyun Liu","10.1101/2022.03.25.485859","","Yes","","Large-scale untargeted metabolomics studies suffer from individual variation, batch effects and instrument variability, making comparisons of common spectral features across studies difficult. One solution is to compare studies after compound identification. However, compound identification is expensive and time consuming. We successfully identify common spectral features across multiple studies, with a generalizable experimental design approach. First, we included an anchor strain, PD1074, during sample and data collection. Second, we collected data in blocks with multiple controls. These anchors enabled us to successfully integrate three studies of Caenorhabditis elegans for nuclear magnetic resonance (NMR) spectroscopy and liquid chromatography-mass spectrometry (LC-MS) data from five different assays. We found 34% and 14% of features to be significant in LC-MS and NMR, respectively. Between 20-50% of spectral features differ in a mutant and among a set of genetically diverse natural strains, suggesting this reduced set of spectral features are excellent targets for compound identification.","In this article , the authors integrate three studies of Caenorhabditis elegans for nuclear magnetic resonance (NMR) spectroscopy and liquid chromatography-mass spectrometry (LC-MS) data from five different assays."
"Diagnostics and correction of batch effects in large-scale proteomic studies: a tutorial.","https://scispace.com/papers/diagnostics-and-correction-of-batch-effects-in-large-scale-765qg1vq6u","2021","Journal Article","Molecular Systems Biology","Jelena Cuklina
Jelena Cuklina
Jelena Cuklina
Chloe H. Lee
Evan G. Williams
Evan G. Williams
Tatjana Sajic
Ben C. Collins
Ben C. Collins
María Rodríguez Martínez
Varun S. Sharma
Fabian Wendt
Sandra Goetze
Sandra Goetze
Gregory R. Keele
Bernd Wollscheid
Bernd Wollscheid
Ruedi Aebersold
Ruedi Aebersold
Patrick G. A. Pedrioli","10.15252/MSB.202110240","https://scispace.com/pdf/diagnostics-and-correction-of-batch-effects-in-large-scale-765qg1vq6u.pdf","Yes","87","Advancements in mass spectrometry-based proteomics have enabled experiments encompassing hundreds of samples. While these large sample sets deliver much-needed statistical power, handling them introduces technical variability known as batch effects. Here, we present a step-by-step protocol for the assessment, normalization, and batch correction of proteomic data. We review established methodologies from related fields and describe solutions specific to proteomic challenges, such as ion intensity drift and missing values in quantitative feature matrices. Finally, we compile a set of techniques that enable control of batch effect adjustment quality. We provide an R package, ""proBatch"", containing functions required for each step of the protocol. We demonstrate the utility of this methodology on five proteomic datasets each encompassing hundreds of samples and consisting of multiple experimental designs. In conclusion, we provide guidelines and tools to make the extraction of true biological signal from large proteomic studies more robust and transparent, ultimately facilitating reliable and reproducible research in clinical proteomics and systems biology.","ProBatch as mentioned in this paper is a step-by-step protocol for the assessment, normalization, and batch correction of proteomic data, which is based on a set of techniques that enable control of batch effect adjustment quality."
"Long-Term Metabolomics Reference Material.","https://scispace.com/papers/long-term-metabolomics-reference-material-106099sw5h","2021","Journal Article","Analytical Chemistry","Goncalo J. Gouveia
Amanda O. Shaver
Brianna M. Garcia
Alison M. Morse
Erik C. Andersen
Arthur S. Edison
Lauren M. McIntyre","10.1021/ACS.ANALCHEM.1C01294","","Yes","15","The use of quality control samples in metabolomics ensures data quality, reproducibility, and comparability between studies, analytical platforms, and laboratories. Long-term, stable, and sustainable reference materials (RMs) are a critical component of the quality assurance/quality control (QA/QC) system; however, the limited selection of currently available matrix-matched RMs reduces their applicability for widespread use. To produce an RM in any context, for any matrix that is robust to changes over the course of time, we developed iterative batch averaging method (IBAT). To illustrate this method, we generated 11 independently grown Escherichia coli batches and made an RM over the course of 10 IBAT iterations. We measured the variance of these materials by nuclear magnetic resonance (NMR) and showed that IBAT produces a stable and sustainable RM over time. This E. coli RM was then used as a food source to produce a Caenorhabditis elegans RM for a metabolomics experiment. The metabolite extraction of this material, alongside 41 independently grown individual C. elegans samples of the same genotype, allowed us to estimate the proportion of sample variation in preanalytical steps. From the NMR data, we found that 40% of the metabolite variance is due to the metabolite extraction process and analysis and 60% is due to sample-to-sample variance. The availability of RMs in untargeted metabolomics is one of the predominant needs of the metabolomics community that reach beyond quality control practices. IBAT addresses this need by facilitating the production of biologically relevant RMs and increasing their widespread use.","The use of quality control samples in metabolomics ensures data quality, reproducibility, and comparability between studies, analytical platforms, and laboratories as discussed by the authors, which is a critical component of the quality assurance/quality control (QA/QC) system; however, the limited selection of currently available matrix matched reference materials reduces their applicability for widespread use."
"Normalizing and Correcting Variable and Complex LC–MS Metabolomic Data with the R Package pseudoDrift","https://scispace.com/papers/normalizing-and-correcting-variable-and-complex-lc-ms-3126bkxq","2022","Journal Article","Metabolites","Jonas Rodriguez
Lina Gomez-Cano
Erich Grotewold
Natalia de Leon","10.3390/metabo12050435","https://scispace.com/pdf/normalizing-and-correcting-variable-and-complex-lc-ms-3126bkxq.pdf","Yes","","In biological research domains, liquid chromatography–mass spectroscopy (LC-MS) has prevailed as the preferred technique for generating high quality metabolomic data. However, even with advanced instrumentation and established data acquisition protocols, technical errors are still routinely encountered and can pose a significant challenge to unveiling biologically relevant information. In large-scale studies, signal drift and batch effects are how technical errors are most commonly manifested. We developed pseudoDrift, an R package with capabilities for data simulation and outlier detection, and a new training and testing approach that is implemented to capture and to optionally correct for technical errors in LC–MS metabolomic data. Using data simulation, we demonstrate here that our approach performs equally as well as existing methods and offers increased flexibility to the researcher. As part of our study, we generated a targeted LC–MS dataset that profiled 33 phenolic compounds from seedling stem tissue in 602 genetically diverse non-transgenic maize inbred lines. This dataset provides a unique opportunity to investigate the dynamics of specialized metabolism in plants.","P pseudoDrift, an R package with capabilities for data simulation and outlier detection, and a new training and testing approach that is implemented to capture and to optionally correct for technical errors in LC–MS metabolomic data are developed."
"Batch correction and harmonization of –Omics datasets with a tunable median polish of ratio","https://scispace.com/papers/batch-correction-and-harmonization-of-omics-datasets-with-a-38s8s79h","2023","Journal Article","Frontiers in Systems Biology","Eric B. Dammer
Nicholas T. Seyfried
Erik C. B. Johnson","10.3389/fsysb.2023.1092341","","Yes","21","Large scale −omics datasets can provide new insights into normal and disease-related biology when analyzed through a systems biology framework. However, technical artefacts present in most −omics datasets due to variations in sample preparation, batching, platform settings, personnel, and other experimental procedures prevent useful analyses of such data without prior adjustment for these technical factors. Here, we demonstrate a tunable median polish of ratio (TAMPOR) approach for batch effect correction and agglomeration of multiple, multi-batch, site-specific cohorts into a single analyte abundance data matrix that is suitable for systems biology analyses. We illustrate the utility and versatility of TAMPOR through four distinct use cases where the method has been applied to different proteomic datasets, some of which contain a specific defect that must be addressed prior to analysis. We compare quality control metrics and sources of variance before and after application of TAMPOR to show that TAMPOR is effective at removing batch effects and other unwanted sources of variance in −omics data. We also show how TAMPOR can be used to harmonize −omics datasets even when the data are acquired using different analytical approaches. TAMPOR is a powerful and flexible approach for cleaning and harmonization of −omics data prior to downstream systems biology analysis.","In this article , a tunable median polish of ratio (TAMPOR) approach is proposed for batch effect correction and agglomeration of multiple, multi-batch, site-specific cohorts into a single analyte abundance data matrix."
"Assessing and mitigating batch effects in large-scale multiomics studies","https://scispace.com/papers/assessing-and-mitigating-batch-effects-in-large-scale-fqpe6uyy7b","2023","Dataset","","Yibin Ying","10.6084/m9.figshare.23605833","","No","","<strong>Quartet RNA reference materials and multi-batch RNA-seq dataset</strong> The Quartet RNA reference materials were derived from the Epstein-Barr Virus (EBV) immortalized B-lymphoblastoid cell lines from four members of a Chinese family quartet including monozygotic twin daughters (D5 and D6), father (F7), and mother (M8). RNA-seq datasets from the Quartet RNA reference materials were then collected, consisting of 252 RNA-seq libraries from 21 batches generated in eight labs using two library construction protocols (PolyA selection and RiboZero) and two sequencing platforms (Illumina NovaSeq (ILM) and MGI DNBSEQ-T7 (BGI)). Here, a batch is defined as 12 libraries from a standard sample set, consisting of 12 vials with each representing one of the triplicates of the Quartet RNA reference sample groups, whose library construction and sequencing experiments were conducted simultaneously. To facilitate the adoption of reference materials, reference datasets, and quality metrics from the Quartet Project, we developed a Quartet Data Portal (http://chinese-quartet.org/) for access to the Quartet resources and enhancing quality consciousness of the community. Researchers can request the reference materials, datasets, and reference datasets from the data portal. <br> <strong>Dataset used in this study</strong> A subset of datasets from Quartet RNA reference materials for illustrating visualization tools in terms of diagnostics of batch effects. This example dataset includes 27 libraries from three RNA-seq batches. Different numbers of replicates (n=5~9) of reference materials are included in each batch to mimic a confounded scenario that replicates of reference materials are not equally distributed across batches. <br> <strong>Data analysis</strong> Expression matrix in log2-transformed Fragments Per Kilobase of transcript per Million mapped reads (FPKM) values were used as expression profiles before batch correction. Expression profiles based on detected genes were used for further analysis. A gene was considered detectable (expressed) in a biological group within a batch if ≥ 3 reads were mapped onto it in at least two of the three replicates. Ratio-based scaling was conducted for batch correction. Specifically, ratio-based scaling were calculated based on log2FPKM values. For each gene, the mean of expression profiles of replicates of reference sample(s) (e.g. D6) was first calculated, and then subtracted from the log2FPKM values of that gene in each study.","The Quartet RNA reference materials and multi-batch RNA-seq dataset provide a valuable resource for assessing and mitigating batch effects in large-scale multiomics studies. The dataset includes a subset of datasets from the Quartet RNA reference materials and can be used to illustrate visualization tools for diagnostics of batch effects."
"DBnorm as an R package for the comparison and selection of appropriate statistical methods for batch effect correction in metabolomic studies.","https://scispace.com/papers/dbnorm-as-an-r-package-for-the-comparison-and-selection-of-174t47bhk3","2021","Journal Article","Scientific Reports","Nasim Bararpour
Federica Gilardi
Cristian Carmeli
Cristian Carmeli
Jonathan Sidibé
Julijana Ivanisevic
Tiziana Caputo
Marc Augsburger
Silke Grabherr
Béatrice Desvergne
Nicolas Guex
Murielle Bochud
Aurélien Thomas","10.1038/S41598-021-84824-3","https://serval.unil.ch/resource/serval:BIB_C00E7D1EAE17.P001/REF.pdf","Yes","18","As a powerful phenotyping technology, metabolomics provides new opportunities in biomarker discovery through metabolome-wide association studies (MWAS) and the identification of metabolites having a regulatory effect in various biological processes. While mass spectrometry-based (MS) metabolomics assays are endowed with high throughput and sensitivity, MWAS are doomed to long-term data acquisition generating an overtime-analytical signal drift that can hinder the uncovering of real biologically relevant changes. We developed ""dbnorm"", a package in the R environment, which allows for an easy comparison of the model performance of advanced statistical tools commonly used in metabolomics to remove batch effects from large metabolomics datasets. ""dbnorm"" integrates advanced statistical tools to inspect the dataset structure not only at the macroscopic (sample batches) scale, but also at the microscopic (metabolic features) level. To compare the model performance on data correction, ""dbnorm"" assigns a score that help users identify the best fitting model for each dataset. In this study, we applied ""dbnorm"" to two large-scale metabolomics datasets as a proof of concept. We demonstrate that ""dbnorm"" allows for the accurate selection of the most appropriate statistical tool to efficiently remove the overtime signal drift and to focus on the relevant biological components of complex datasets.","Dbnorm as mentioned in this paper integrates advanced statistical tools to inspect the dataset structure not only at the macroscopic (sample batches) scale, but also at the microscopic (metabolic features) level."
"MRMQuant: Automated MRM Data Quantitation for Large-Scale Targeted Metabolomics Analysis","https://scispace.com/papers/mrmquant-automated-mrm-data-quantitation-for-large-scale-4x1iivl08sfg","2024","Journal Article","Analytical Chemistry","Ke‐Shiuan Lynn
Hsiang-Yu Tang
Chi-Jen Lo
Cheng-Hung Yang
Yi‐Ting Tseng
Mei‐Ling Cheng","10.1021/acs.analchem.4c02462","","No","","Multiple reaction monitoring (MRM) is a powerful and popular technique used for metabolite quantification in targeted metabolomics. Accurate and consistent quantitation of metabolites from the MRM data is essential for subsequent analyses. Here, we developed an automated tool, MRMQuant, for targeted metabolomic quantitation using high-throughput liquid chromatography-tandem mass spectrometry MRM data to provide users with an easy-to-use tool for accurate MRM data quantitation with minimal human intervention. This tool has many user-friendly functions and features to inspect and correct the quantitation results as required. MRMQuant possesses the following features to ensure accurate quantitation: (1) dynamic signal smoothing, (2) automatic deconvolution of coeluted peaks, (3) absolute quantitation via standard curves and/or internal standards, (4) visualized inspection and correction, (5) corrections applicable to multiple samples, and (6) batch-effect correction.","MRMQuant is an automated tool for accurate targeted metabolomics quantitation using high-throughput MRM data, featuring dynamic signal smoothing, peak deconvolution, absolute quantitation, and batch-effect correction with minimal human intervention."
"Perspectives for better batch effect correction in mass-spectrometry-based proteomics","https://scispace.com/papers/perspectives-for-better-batch-effect-correction-in-mass-10xb9jdz","2022","Journal Article","Computational and structural biotechnology journal","Ser-Xian Phua
Kai Peng Lim
Wilson Wen Bin Goh","10.1016/j.csbj.2022.08.022","https://doi.org/10.1016/j.csbj.2022.08.022","Yes","13","Mass-spectrometry-based proteomics presents some unique challenges for batch effect correction. Batch effects are technical sources of variation, can confound analysis and usually non-biological in nature. As proteomic analysis involves several stages of data transformation from spectra to protein, the decision on when and what to apply batch correction on is often unclear. Here, we explore several relevant issues pertinent to batch effect correct considerations. The first involves applications of batch effect correction requiring prior knowledge on batch factors and exploring data to uncover new/unknown batch factors. The second considers recent literature that suggests there is no single best batch effect correction algorithm---i.e., instead of a best approach, one may instead ask, what is a suitable approach. The third section considers issues of batch effect detection. And finally, we look at potential developments for proteomic-specific batch effect correction methods and how to do better functional evaluations on batch corrected data.","In this paper , the authors explore several relevant issues pertinent to batch effect correct considerations, including applications of batch effect correction requiring prior knowledge on batch factors and exploring data to uncover new/unknown batch factors."